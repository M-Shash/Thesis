{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c296d2-6f05-444c-bfe1-86cd4b0e0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "from flwr.server.strategy.strategy import Strategy\n",
    "from flwr.common import Context, ndarrays_to_parameters, FitIns, EvaluateIns, parameters_to_ndarrays, NDArrays\n",
    "from flwr.client import NumPyClient\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms,datasets\n",
    "from torchvision.models import  inception_v3\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image, ImageDraw\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import uuid\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as UMAP\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math \n",
    "import cv2 ,sys\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from tqdm import tqdm\n",
    "import clip as clip\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('spawn', force=True)\n",
    "from itertools import chain\n",
    "import matplotlib.cm as cm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb54655-985f-4803-9082-e116aad3c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e98825a0-e492-4722-a739-5edaffd46b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)#ViT-B/32 ViT-L/14\n",
    "target_classes = [\"basketball\",\"moving van\",\"mountain bike\",\"all-terrain bike\",\"van\",\"off-roader\"]\n",
    "text_tokens = clip.tokenize(target_classes).to(device)\n",
    "DATASET_TO_CLIP = {\n",
    "    \"basketball\": {\"basketball\"},\n",
    "    \"mountain bike\": {\"mountain bike\", \"all-terrain bike\", \"off-roader\"},\n",
    "    \"moving van\": {\"moving van\"},\n",
    "}\n",
    "with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "# === 1. Image Folder Dataset + SAM2 Segment Generation === #\n",
    "\n",
    "\n",
    "#target_classes = [\n",
    "#    \"soccer ball\",\n",
    "#    \"football\",\n",
    "#    \"football match\",\n",
    "#    \"mountain bike\",\n",
    "#    \"racing bicycle\",\n",
    "#    \"bike\",\n",
    "#    \"cornears\",\n",
    "#    \"A kernel of corn\",\n",
    "#    \"corn on the cob\",\n",
    "#    \"an ear of corn\",\n",
    "#    \"corn plant\",\n",
    "#    \"police van\",\n",
    "#    \"lionfish\",\n",
    "#    \"colorful lionfish\",\n",
    "#    \"basketball\",\n",
    "#    \"orange basketball\",\n",
    "#    \"basketball players\",\n",
    "#    \"basketball net\",\n",
    "#    \"basketball clothes\",\n",
    "#    \"van\",\n",
    "#    \"police\",\n",
    "#    \"truck\",\n",
    "#    \"car\",\n",
    "#    \"fish\",\n",
    "#    \"lion fish\",\n",
    "#    \"striped venomous fish\",\n",
    "#    \"marine fish with long spiny fins\",\n",
    "#    \"zebra-patterned reef fish\",\n",
    "#    \"fish with feather-like fins\",\n",
    "#    \"fish with fins\"\n",
    "#]\n",
    "\n",
    "def numpy_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: numpy_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [numpy_to_native(i) for i in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return [numpy_to_native(i) for i in obj]  # Convert tuples to lists for JSON\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "        \n",
    "def get_bbox_from_mask(mask):\n",
    "    ys, xs = np.where(mask)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return None  # skip empty mask\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    return (x0, y0, x1, y1)\n",
    "    \n",
    "\n",
    "def save_segments_with_metadata(image_id, original_label, segments, scores,bboxes, orig_img_path,output_dir, score_threshold):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    metadata = []\n",
    "\n",
    "    for i, (segment, score, bboxes) in enumerate(zip(segments, scores, bboxes)):\n",
    "        if score < score_threshold:\n",
    "            continue\n",
    "\n",
    "        original_name = os.path.splitext(os.path.basename(orig_img_path))[0]\n",
    "        seg_filename = f\"{original_name}_{i}.png\"\n",
    "        seg_path = os.path.join(output_dir, seg_filename)\n",
    "        Image.fromarray(segment).save(seg_path)\n",
    "\n",
    "        metadata.append({\n",
    "            \"segment_id\": seg_filename,\n",
    "            \"original_label\": int(original_label),\n",
    "            \"original_image_index\": int(image_id),\n",
    "            \"score\": float(score),\n",
    "            \"bbox\": bboxes,\n",
    "            \"original_image_path\": orig_img_path\n",
    "        })\n",
    "\n",
    "    meta_path = os.path.join(output_dir, \"metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            existing_metadata = json.load(f)\n",
    "        existing_metadata.extend(metadata)\n",
    "        metadata = existing_metadata\n",
    "\n",
    "    with open(meta_path, \"w\") as f:\n",
    "        json.dump(numpy_to_native(metadata), f, indent=2)\n",
    "\n",
    "    return meta_path\n",
    "\n",
    "\n",
    "# === 2. Segment Dataset with CLIP Filtering === #\n",
    "def ensure_pil(img):\n",
    "    if not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img)\n",
    "    return img\n",
    "\n",
    "def clip_filter_segment(seg_img_pil):\n",
    "    \n",
    "    img_clip = clip_preprocess(seg_img_pil).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_feature = clip_model.encode_image(img_clip)\n",
    "        image_feature /= image_feature.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_feature @ text_features.T)\n",
    "        probs = similarity.softmax(dim=-1)\n",
    "        topk = 2\n",
    "        top_scores, top_indices = torch.topk(probs, k=topk, dim=-1)\n",
    "        #best_idx = probs.argmax(dim=-1).item()\n",
    "        clip_scores = [s.item() for s in top_scores[0]]\n",
    "        class_names = [target_classes[i] for i in top_indices[0].tolist()]\n",
    "        # Empirical threshold to reduce false positives\n",
    "        #threshold = 0.5\n",
    "        return class_names, clip_scores# if clip_score >= threshold else (None, clip_score)\n",
    "\n",
    "\n",
    "class SegmentDataset(Dataset):\n",
    "    def __init__(self, segment_root, transform):\n",
    "        self.samples = []\n",
    "        self.metadata = []\n",
    "        self.total_segments = 0\n",
    "        self.clip_filtered_out = 0\n",
    "        self.transform = transform\n",
    "\n",
    "        for class_name in sorted(os.listdir(segment_root)):\n",
    "            meta_path = os.path.join(segment_root, class_name, \"metadata.json\")\n",
    "            if not os.path.exists(meta_path):\n",
    "                print(f\"[DEBUG] Metadata json missing for class: {class_name}\")\n",
    "                continue\n",
    "\n",
    "            with open(meta_path, \"r\") as f:\n",
    "                meta = json.load(f)\n",
    "\n",
    "            for entry in meta:\n",
    "                img_path = os.path.join(segment_root, class_name, entry[\"segment_id\"])\n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"[DEBUG] Missing segment image file: {img_path}\")\n",
    "                    continue\n",
    "\n",
    "                self.total_segments += 1\n",
    "\n",
    "                # Do filtering at init time\n",
    "                try:\n",
    "                    pil_img = Image.open(img_path).convert(\"RGB\")\n",
    "                    top_classes, top_scores = clip_filter_segment(pil_img)\n",
    "                    valid_clip_classes = DATASET_TO_CLIP.get(class_name, set())\n",
    "                    valid_clip_classes_lower = {c.lower() for c in valid_clip_classes}\n",
    "                    accepted = False\n",
    "                    original_class = class_name\n",
    "                    for c, s in zip(top_classes, top_scores):\n",
    "                        if c.lower() in valid_clip_classes_lower and s >= 0.5:\n",
    "                            clip_class = c\n",
    "                            clip_score = s\n",
    "                            accepted = True\n",
    "                            break\n",
    "                    if not accepted:\n",
    "                       self.clip_filtered_out += 1\n",
    "                       continue\n",
    "                    if clip_class is None:\n",
    "                        self.clip_filtered_out += 1\n",
    "                        continue  # skip this sample\n",
    "                    \n",
    "                        \n",
    "                    entry.update({\n",
    "                        \"class_name\": class_name,\n",
    "                        \"img_path\": img_path,\n",
    "                        \"clip_class\": clip_class,\n",
    "                        \"clip_score\": clip_score\n",
    "                    })\n",
    "                    self.samples.append(img_path)\n",
    "                    self.metadata.append(entry)\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Failed filtering {img_path}: {e}\")\n",
    "                    self.clip_filtered_out += 1\n",
    "                    continue\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.samples[idx]\n",
    "        meta = self.metadata[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = torch.tensor(meta[\"original_label\"])\n",
    "        return img, label, meta\n",
    "\n",
    "    def get_stats(self):\n",
    "        return {\n",
    "            \"total_attempted\": self.total_segments,\n",
    "            \"clip_filtered_out\": self.clip_filtered_out,\n",
    "            \"clip_passed\": len(self.samples)\n",
    "        }\n",
    "\n",
    "dino_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def custom_collate(batch):\n",
    "    batch = [item for item in batch if item is not None and all(e is not None for e in item)]\n",
    "    if not batch:\n",
    "        return torch.empty(0), torch.empty(0), []\n",
    "    images, labels, metas = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    metas = list(metas)\n",
    "    return images, labels, metas\n",
    "@torch.no_grad()\n",
    "def extract_features_dinov2(dataloader, model, device=\"cuda\"):\n",
    "    try:\n",
    "        features, labels_all, metas_all = [], [], []\n",
    "        for imgs, labels, meta_batch in tqdm(dataloader, desc=\"Extracting DINOv2 features\"):\n",
    "            if imgs.size(0) == 0:\n",
    "                continue \n",
    "            imgs = imgs.to(device)\n",
    "            feats = model(imgs).cpu()\n",
    "            features.append(feats)\n",
    "            labels_all.append(labels)\n",
    "            metas_all.extend(meta_batch)\n",
    "        features = torch.cat(features)\n",
    "        labels_all = torch.cat(labels_all)\n",
    "        return features, labels_all, metas_all\n",
    "    except Exception as e:\n",
    "        print(f\"Client can't extract the Dino features because of : {e}\")\n",
    "        # Return empty or None values explicitly to avoid implicit None\n",
    "        return torch.empty(0), torch.empty(0), []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49214e0c-64fe-4b7c-9bf7-ba943dc2b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 4. Clustering & Visualization  === #\n",
    "def denormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean, dtype=tensor.dtype, device=tensor.device).view(-1,1,1)\n",
    "    std = torch.tensor(std, dtype=tensor.dtype, device=tensor.device).view(-1,1,1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# -- 3. Crop to mask/segment content (removes almost all background) --\n",
    "def crop_to_foreground(img_np, bg_val=0, margin=4):\n",
    "    # Assumes background is black (0)\n",
    "    mask = np.any(img_np != bg_val, axis=-1)\n",
    "    coords = np.argwhere(mask)\n",
    "    if coords.size == 0:\n",
    "        return img_np\n",
    "    y0, x0 = np.maximum(coords.min(axis=0) - margin, 0)\n",
    "    y1, x1 = np.minimum(coords.max(axis=0) + 1 + margin, img_np.shape[:2])\n",
    "    return img_np[y0:y1, x0:x1]\n",
    "\n",
    "# -- 4. Resize with preserved aspect ratio for neat thumbnails --\n",
    "def resize_with_aspect(img_np, max_size=(128, 128)):\n",
    "    img_pil = Image.fromarray(img_np if img_np.max() > 1 else (img_np * 255).astype('uint8'))\n",
    "    img_pil = img_pil.convert(\"RGB\")\n",
    "    img_pil.thumbnail(max_size, Image.LANCZOS)\n",
    "    return np.asarray(img_pil)\n",
    "\n",
    "# -- 5. Visualization for each cluster --\n",
    "def visualize_cluster_segments(cluster_meta_list, cluster_id, max_visualize=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.suptitle(f\"Cluster {cluster_id}, Size: {len(cluster_meta_list)}\", fontsize=16, y=1.08)\n",
    "    for i, meta in enumerate(cluster_meta_list[:max_visualize]):\n",
    "        img_path = meta[\"img_path\"]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_np = np.array(img)\n",
    "            img_np_cropped = crop_to_foreground(img_np)\n",
    "            img_np_display = resize_with_aspect(img_np_cropped, max_size=(128,128))\n",
    "            plt.subplot(1, max_visualize, i+1)\n",
    "            plt.imshow(img_np_display)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Cls: {meta['original_label']}\\nScore: {meta['score']:.2f}\\nImgID: {meta['original_image_index']}\\n Clip class: {meta['clip_class']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            plt.subplot(1, max_visualize, i+1)\n",
    "            plt.text(0.5, 0.5, 'Image Error', ha='center', va='center')\n",
    "            plt.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7f3a156-cf58-401c-8918-8e50cec6f36f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_arrow_on_image(ax, xy_from, xy_to, color='yellow', width=2, mutation_scale=15):\n",
    "    \"\"\"Draws an arrow from xy_from to xy_to on a matplotlib axis.\"\"\"\n",
    "    ax.annotate(\n",
    "        '', xy=xy_to, xytext=xy_from,\n",
    "        arrowprops=dict(\n",
    "            arrowstyle='->',\n",
    "            color=color,\n",
    "            lw=width,\n",
    "            shrinkA=0,\n",
    "            shrinkB=0,\n",
    "            mutation_scale=mutation_scale\n",
    "        ),\n",
    "        annotation_clip=False\n",
    "    )\n",
    "\n",
    "def create_mask_overlay_from_original(original_img_path, mask_img, alpha=0.6, mask_color=(255, 180, 0)):\n",
    "    \"\"\" Overlay a binary mask onto the original full image.\"\"\"\n",
    "    orig = np.array(Image.open(original_img_path).convert(\"RGB\")).copy()\n",
    "    mask = np.array(mask_img)\n",
    "    if mask.max() > 1:\n",
    "        mask_bin = (mask[..., 0] > 0).astype(np.uint8)\n",
    "    else:\n",
    "        mask_bin = mask[..., 0]\n",
    "    mask_rgb = np.zeros_like(orig)\n",
    "    mask_rgb[mask_bin == 1] = mask_color\n",
    "    out = cv2.addWeighted(mask_rgb, alpha, orig, 1 - alpha, 0)\n",
    "    return Image.fromarray(out)\n",
    "\n",
    "def visualize_clusters_with_overlay_and_arrow(clusters_by_class, max_per_cluster=3):\n",
    "    \"\"\" Visualizes each cluster with segment (left) and overlay on original image (right), with arrow.\"\"\"\n",
    "    for class_name, cluster_info_list in clusters_by_class.items():\n",
    "        n_clusters = len(cluster_info_list)\n",
    "        fig, axes = plt.subplots(n_clusters, max_per_cluster, figsize=(5 * max_per_cluster, 4 * n_clusters))\n",
    "        if n_clusters == 1:\n",
    "            axes = np.expand_dims(axes, 0)\n",
    "        plt.suptitle(f\"Class: {class_name} Clusters (sorted by CLIP saliency)\", fontsize=16)\n",
    "\n",
    "        for r, (cluster, info) in enumerate(cluster_info_list):\n",
    "            for c in range(max_per_cluster):\n",
    "                ax = axes[r, c] if n_clusters > 1 else axes[0, c]\n",
    "                try:\n",
    "                    meta = cluster[c]\n",
    "                except IndexError:\n",
    "                    ax.axis('off')\n",
    "                    continue\n",
    "\n",
    "                # Load the segment image as saved (RGB)\n",
    "                seg_img = Image.open(meta['img_path']).convert(\"RGB\")\n",
    "                seg_np = np.array(seg_img)\n",
    "\n",
    "                # Derive binary mask (used only for overlay)\n",
    "                gray = cv2.cvtColor(seg_np, cv2.COLOR_RGB2GRAY)\n",
    "                mask_only = np.where(gray > 5, 255, 0).astype(np.uint8)\n",
    "                mask_only = np.expand_dims(mask_only, axis=-1)\n",
    "\n",
    "                # Load the original image and create overlay\n",
    "                orig_img_path = meta['original_image_path']\n",
    "                overlay_img = create_mask_overlay_from_original(orig_img_path, mask_only, alpha=0.6)\n",
    "\n",
    "                # Resize both views\n",
    "                thumb_segment = seg_img.resize((64, 64), Image.LANCZOS)\n",
    "                thumb_overlay = overlay_img.resize((128, 128), Image.LANCZOS)\n",
    "\n",
    "                # Compose side-by-side image\n",
    "                combined = Image.new('RGB', (thumb_segment.width + thumb_overlay.width, thumb_overlay.height), (255, 255, 255))\n",
    "                combined.paste(thumb_segment, (0, 0))\n",
    "                combined.paste(thumb_overlay, (thumb_segment.width, 0))\n",
    "\n",
    "                # Show in subplot\n",
    "                ax.imshow(combined)\n",
    "                ax.axis('off')\n",
    "\n",
    "                # Draw arrow between segment and overlay\n",
    "                arrow_start = (thumb_segment.width // 2, thumb_overlay.height // 2)\n",
    "                arrow_end = (thumb_segment.width + thumb_overlay.width // 2, thumb_overlay.height // 2)\n",
    "                draw_arrow_on_image(ax, arrow_start, arrow_end)\n",
    "\n",
    "                # Add title with cluster info\n",
    "                ax.set_title(f\"CID: {info['cluster_id']} | Score: {meta['clip_score']:.2f} \\n Clip Interpretation:{meta['clip_class']}\", fontsize=9)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ed945-a40d-4a87-9605-1b532c62df68",
   "metadata": {},
   "source": [
    "### Federated server strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd192bf8-1439-4210-a151-f835e20f785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedKMeansStrategy(Strategy):\n",
    "    def __init__(self, initial_centroids, num_clusters=10, tol=1e-3):\n",
    "        self.global_centroids = initial_centroids\n",
    "        self.num_clusters = num_clusters\n",
    "        self.tol = tol\n",
    "        self.converged = False\n",
    "\n",
    "    def initialize_parameters(self, client_manager):\n",
    "        return ndarrays_to_parameters([])\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        print(f\"[Strategy] configure_fit called for round {server_round}\")\n",
    "        print(f\"[Strategy] Available clients: {len(client_manager.all())}\")\n",
    "        if len(client_manager.all()) == 0:\n",
    "            return []\n",
    "\n",
    "        mode = \"tcav\" if self.converged else \"cluster\"\n",
    "        config = {\"mode\": mode}\n",
    "        #if not self.converged:\n",
    "        serialized = {int(k): v.tolist() for k, v in self.global_centroids.items()}\n",
    "        config[\"centroids\"] = json.dumps(serialized) \n",
    "\n",
    "        dummy = ndarrays_to_parameters([np.zeros((1, 1), dtype=np.float32)])\n",
    "        fit_ins = FitIns(parameters=dummy, config=config)\n",
    "        return [(client, fit_ins) for client in client_manager.all().values()]\n",
    "\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if failures:\n",
    "            print(f\"[Server] {len(failures)} client failures in round {server_round}:\")\n",
    "            for failure in failures:\n",
    "                print(f\"  - Failure: {str(failure)}\")\n",
    "\n",
    "        if self.converged or not results:\n",
    "            dummy = ndarrays_to_parameters([np.zeros((1, 1), dtype=np.float32)])\n",
    "            return dummy, {}\n",
    "\n",
    "        all_cluster_data = []\n",
    "        per_class_clusters = defaultdict(list)  # class_label -> list of (centroid, count)\n",
    "        for client_proxy, fit_res in results:\n",
    "            #cluster_data_json = fit_res.metrics.get(\"cluster_data\")\n",
    "            arrays = parameters_to_ndarrays(fit_res.parameters)\n",
    "            #if cluster_data_json is None:\n",
    "            #    continue\n",
    "            if len(arrays) != 4:\n",
    "                print(\"Malformed client parameters\")\n",
    "                continue\n",
    "            centroids, counts, class_labels, cluster_indices = arrays\n",
    "            #cluster_data_dict = json.loads(cluster_data_json)\n",
    "            for centroid, count, class_label, cl_idx in zip(centroids, counts, class_labels, cluster_indices):\n",
    "                        per_class_clusters[int(class_label)].append((centroid, count, int(cl_idx)))\n",
    "\n",
    "        new_global_centroids = {}\n",
    "        for class_label, centroid_list in per_class_clusters.items():\n",
    "            if not centroid_list:\n",
    "                continue\n",
    "            n_clusters = len(centroid_list)\n",
    "            # Organize by cluster index if it is present in entry, else assign sequentially\n",
    "            sum_centroids = None\n",
    "            total_counts = None\n",
    "            K = self.num_clusters if hasattr(self, \"num_clusters\") else n_clusters\n",
    "            sum_centroids = np.zeros((K, centroid_list[0][0].shape[0]), dtype=np.float32)\n",
    "            total_counts = np.zeros(K, dtype=np.float32)\n",
    "            for i, (centroid, count,cluster_index) in enumerate(centroid_list):\n",
    "                idx = i % K\n",
    "                sum_centroids[idx] += centroid * count\n",
    "                total_counts[idx] += count\n",
    "            for idx in range(K):\n",
    "                if total_counts[idx] > 0:\n",
    "                    sum_centroids[idx] /= total_counts[idx]\n",
    "            new_global_centroids[class_label] = sum_centroids\n",
    "\n",
    "        max_shift = 0.0\n",
    "        for class_label in new_global_centroids:\n",
    "            if class_label in self.global_centroids:\n",
    "                shift = np.linalg.norm(self.global_centroids[class_label] - new_global_centroids[class_label], axis=1)\n",
    "                max_shift = max(max_shift, np.max(shift))\n",
    "        self.global_centroids = new_global_centroids\n",
    "        #shift = np.linalg.norm(self.global_centroids - new_centroids, axis=1)\n",
    "        #max_shift = np.max(shift)\n",
    "        print(f\"[Round {server_round}] Max centroid shift: {max_shift:.2f}\")\n",
    "        if max_shift < self.tol:\n",
    "            print(\"Convergence achieved. Switching to TCAV phase.\")\n",
    "            self.converged = True\n",
    "\n",
    "        dummy = ndarrays_to_parameters([np.zeros((1, 1), dtype=np.float32)])\n",
    "        return dummy, {}\n",
    "\n",
    "    def evaluate(self, server_round, parameters):\n",
    "        print(f\"[Server] Global evaluation for round {server_round} skipped (no global model).\")\n",
    "        return 0.0, {}\n",
    "\n",
    "    def configure_evaluate(self, server_round, parameters, client_manager):\n",
    "        mode = \"tcav\" if self.converged else \"cluster\"\n",
    "        config = {\"mode\": mode}\n",
    "        serialized = {int(k): v.tolist() for k, v in self.global_centroids.items()}  # Serialize to JSON string\n",
    "        config = {\n",
    "        \"mode\": mode,\n",
    "        \"centroids\": json.dumps(serialized)\n",
    "    }\n",
    "        dummy = ndarrays_to_parameters([np.zeros((1, 1), dtype=np.float32)])\n",
    "        evaluate_ins = EvaluateIns(parameters=dummy, config=config)\n",
    "        return [(client, evaluate_ins) for client in client_manager.all().values()]\n",
    "\n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        return 0.0, {}\n",
    "        if not results:\n",
    "            return 0.0, {}\n",
    "\n",
    "        total_samples = 0\n",
    "        total_inertia = 0.0\n",
    "        all_cluster_sizes = defaultdict(lambda: defaultdict(int))  # class_label -> cluster_idx -> count\n",
    "\n",
    "        for client, client_res in results:\n",
    "            metrics = client_res.metrics\n",
    "            num_samples = client_res.num_examples\n",
    "            # Inertia is per class (sum over clusters within class)\n",
    "            if \"inertia\" in metrics:\n",
    "                class_inertia = json.loads(metrics[\"inertia\"])\n",
    "                for class_label, inertia in class_inertia.items():\n",
    "                    total_inertia += float(inertia)\n",
    "                    total_samples += num_samples  # You can sum per sample or per class\n",
    "\n",
    "            if \"cluster_sizes\" in metrics:\n",
    "                cluster_sizes = json.loads(metrics[\"cluster_sizes\"])\n",
    "                for class_label, clusters in cluster_sizes.items():\n",
    "                    for cluster_idx, sz in clusters.items():\n",
    "                        all_cluster_sizes[int(class_label)][int(cluster_idx)] += sz\n",
    "\n",
    "        avg_inertia = total_inertia / total_samples if total_samples else 0.0\n",
    "        print(f\"\\n[Server][Evaluation Round {server_round}]\")\n",
    "        print(f\"  > Avg Inertia: {avg_inertia:.2f}\")\n",
    "        for class_label, c_sizes in all_cluster_sizes.items():\n",
    "            print(f\"  > Class {class_label} cluster sizes: {dict(c_sizes)}\")\n",
    "        metrics = {\n",
    "            \"avg_inertia\": avg_inertia,\n",
    "            \"global_cluster_sizes\": {cl: dict(c_sizes) for cl, c_sizes in all_cluster_sizes.items()}\n",
    "        }\n",
    "        return total_samples, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc36e76-d9b5-4a67-b1bb-53a621ba0028",
   "metadata": {},
   "source": [
    "### Federated Client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f33b95-02a7-44a6-a7b4-f8d12dbde01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThesisKMeansClient(NumPyClient):\n",
    "    def __init__(self, cid, features, labels, metas, device):\n",
    "        print(f\"[Client] {cid} Initializing with features: {type(features)}, shape: {getattr(features, 'shape', 'no shape')}\")\n",
    "        print(f\"[Client] {cid} Labels: {type(labels)}, length: {len(labels) if hasattr(labels, '__len__') else 'no length'}\")\n",
    "        print(f\"[Client] {cid} Metas: {type(metas)}, length: {len(metas) if hasattr(metas, '__len__') else 'no length'}\")\n",
    "    \n",
    "        # Validate data\n",
    "        if hasattr(features, 'shape') and features.shape[0] == 0:\n",
    "            raise ValueError(\"Features array is empty\")\n",
    "        if len(labels) == 0:\n",
    "            raise ValueError(\"Labels list is empty\")\n",
    "        if len(metas) == 0:\n",
    "            raise ValueError(\"Metas list is empty\")\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.metas = metas\n",
    "        self.k = 15\n",
    "        self.cluster_labels = None\n",
    "        self.cluster_cache = None\n",
    "        self.dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device).eval()\n",
    "        self.device = device\n",
    "        self.current_centroid = None\n",
    "        self.cid = cid\n",
    "        self.label_to_classname = { 0:'basketball', 1: 'mountain bike', 2: 'moving van'}\n",
    "        class_feature_map = defaultdict(list)\n",
    "        class_meta_map = defaultdict(list)\n",
    "        self.tcav_ran =False\n",
    "    def get_parameters(self, config):\n",
    "        return []\n",
    "        \n",
    "    #def save_state(self):\n",
    "    #    import pickle\n",
    "    #    from collections import defaultdict\n",
    "\n",
    "    #    state = {\n",
    "    #    \"features\": self.features,\n",
    "    #    \"labels\": self.labels,\n",
    "    #    \"metas\": self.metas,\n",
    "    #    \"cluster_labels\": getattr(self, \"cluster_labels\", None)#,\n",
    "    #    #\"class_feature_map\": self.class_feature_map,  \n",
    "    #    #\"class_meta_map\": self.class_meta_map,\n",
    "    #    }\n",
    "    #    with open(f\"segment_dataset/segments/segments_client_{self.cid}/client_{self.cid}_state.pkl\", \"wb\") as f:\n",
    "    #        pickle.dump(state, f)\n",
    "    #    print(f\"[Client {self.cid}] State saved to disk.\")\n",
    "    \n",
    "    #def load_state(self):\n",
    "    #    import pickle\n",
    "    #    state_file = f'segment_dataset/segments/segments_client_{self.cid}/client_{self.cid}_state.pkl'\n",
    "    #    if os.path.exists(state_file):\n",
    "    #        try:\n",
    "    #            with open(state_file, 'rb') as f:\n",
    "    #                if os.path.getsize(state_file) == 0:\n",
    "    #                    raise EOFError(\"State file is empty\")\n",
    "    #                state = pickle.load(f)\n",
    "    #            self.features = state['features']\n",
    "    #            self.labels = state['labels']\n",
    "    #            self.metas = state['metas']\n",
    "                \n",
    "    #            print(f\"[Client {self.cid}] State loaded from {state_file}\")\n",
    "    #        except (EOFError, pickle.UnpicklingError) as e:\n",
    "    #            print(f\"[Client {self.cid}] Failed to load state from {state_file}: {e}. Deleting corrupted file and starting fresh.\")\n",
    "    #            os.remove(state_file)  # Remove bad file\n",
    "    #            self.features  = None\n",
    "    #            self.labels = None\n",
    "    #            self.metas = None\n",
    "    #    else:\n",
    "    #        print(f\"[Client {self.cid}] No state file found; starting fresh\")\n",
    "    #        self.features  = None\n",
    "    #        self.labels = None\n",
    "    #        self.metas = None\n",
    "            \n",
    "    def fit(self, parameters, config) -> tuple[list[np.ndarray], int, dict[str, float | int | str | bytes | bool]]:\n",
    "        try:\n",
    "            from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "            print(\"ðŸ“¦ fit() called on client.\")\n",
    "            mode = config[\"mode\"]\n",
    "            \n",
    "            if mode == \"tcav\":\n",
    "                #self.load_state()\n",
    "                if self.tcav_ran :\n",
    "                    dummy_array = np.zeros((self.k, self.features.shape[1]), dtype=np.float32)\n",
    "                    return [dummy_array], 0, {}\n",
    "                self.tcav_ran= True\n",
    "                class_feature_map = defaultdict(list)\n",
    "                class_meta_map = defaultdict(list)\n",
    "                centroids_dict = json.loads(config[\"centroids\"])\n",
    "                centroids_dict = {int(k): np.array(v, dtype=np.float32) for k, v in centroids_dict.items()}\n",
    "                print(f\" [CLient {self.cid}] is Converged and start constructing the needed Clusters\")\n",
    "            \n",
    "                for f, m in zip(self.features, self.metas):\n",
    "                    class_label = m['original_label']\n",
    "                    class_feature_map[class_label].append(f)\n",
    "                    class_meta_map[class_label].append(m)\n",
    "                #print(f\"Discovered {len(self.class_feature_map)} unique classes\")\n",
    "                for class_label in class_feature_map:\n",
    "                    if class_label not in centroids_dict:\n",
    "                        print(f\"Skipping class {class_label}: no converged centroids from server.\")\n",
    "                        continue\n",
    "                    features_class = np.stack(class_feature_map[class_label])\n",
    "                    metas_class =class_meta_map[class_label]\n",
    "                    centroids = centroids_dict[class_label]\n",
    "                    # Assign each feature to closest cluster in final centroids\n",
    "                    assigned_labels = pairwise_distances_argmin(features_class, centroids)\n",
    "                    for meta, cid in zip(metas_class, assigned_labels):\n",
    "                        meta['cluster_id'] = int(cid)\n",
    "                #self.save_state()\n",
    "                print(f\" [CLient {self.cid}] is Proceeding for Training CAVs and computing TCAV scores...\")\n",
    "                print(f\" [CLient {self.cid}] Picking up the nearest 40 instaces to the clusters' centriods...\")\n",
    "                # === Keep only closest 40 points per cluster and update features/labels/metas =======================================\n",
    "                filtered_metas_global = []                                                                                           #\n",
    "                filtered_indices_global = []                                                                                         #\n",
    "                for class_label in class_feature_map:                                                                                #\n",
    "                    if class_label not in centroids_dict:                                                                            #\n",
    "                        continue                                                                                                     #\n",
    "                    features_class = np.stack(class_feature_map[class_label])                                                        #\n",
    "                    metas_class = class_meta_map[class_label]                                                                        #\n",
    "                    centroids = centroids_dict[class_label]                                                                          #\n",
    "                    assigned_labels = pairwise_distances_argmin(features_class, centroids)                                           #\n",
    "                    for cid in range(centroids.shape[0]):                                                                            #\n",
    "                        idxs = np.where(assigned_labels == cid)[0]                                                                   #\n",
    "                        if len(idxs) == 0:                                                                                           #\n",
    "                            continue                                                                                                 #\n",
    "                        cluster_feats = features_class[idxs]                                                                         #\n",
    "                        dists = np.linalg.norm(cluster_feats - centroids[cid], axis=1)                                               #\n",
    "                        sort_idxs = np.argsort(dists)[:40]                                                                           #\n",
    "                        selected_metas = [metas_class[idxs[i]] for i in sort_idxs]                                                   #\n",
    "                        selected_indices = [idxs[i] for i in sort_idxs]                                                              #\n",
    "                        filtered_metas_global.extend(selected_metas)                                                                 #\n",
    "                        filtered_indices_global.extend([ (class_label, idx) for idx in selected_indices ])                           #\n",
    "                # Rebuild features, labels, metas: (preserving exact order)                                                          #\n",
    "                all_features = []                                                                                                    #\n",
    "                all_labels = []                                                                                                      #\n",
    "                all_metas = []                                                                                                       #\n",
    "                class_to_indices = { cl: np.array([i for i in range(len(class_feature_map[cl]))]) for cl in class_feature_map }      #\n",
    "                for (class_label, idx) in filtered_indices_global:                                                                   #\n",
    "                    all_features.append(class_feature_map[class_label][idx])                                                         #\n",
    "                    all_labels.append(class_label)                                                                                   #\n",
    "                    all_metas.append(class_meta_map[class_label][idx])                                                               #\n",
    "                self.features = np.stack(all_features)                                                                               #\n",
    "                self.labels = np.array(all_labels)                                                                                   #\n",
    "                self.metas = all_metas                                                                                               #\n",
    "                self.train_cavs_and_tcav()\n",
    "                dummy_array = np.zeros((self.k, self.features.shape[1]), dtype=np.float32)\n",
    "                return [dummy_array], 0, {}  # Return NDArrays (list of np.ndarray), int, dict\n",
    "            \n",
    "            if \"centroids\" not in config:\n",
    "                raise ValueError(\"Missing 'centroids' in config\")\n",
    "            silhouette_scores = {}\n",
    "            centroids_dict = json.loads(config[\"centroids\"])\n",
    "            centroids_dict = {int(k): np.array(v, dtype=np.float32) for k, v in centroids_dict.items()}\n",
    "            class_feature_map = defaultdict(list)\n",
    "            class_meta_map = defaultdict(list)\n",
    "            for f, m in zip(self.features, self.metas):\n",
    "                class_label = m['original_label']\n",
    "                class_feature_map[class_label].append(f)\n",
    "                class_meta_map[class_label].append(m)\n",
    "            print(f\"Discovered {len(class_feature_map)} unique classes\")\n",
    "            per_class_cluster_data = {}\n",
    "            for class_label in class_feature_map:\n",
    "                if class_label not in centroids_dict:\n",
    "                    print(f\"[Client {self.cid}]: Skipping class {class_label} (no centroids from server)\")\n",
    "                    continue\n",
    "                print(f\"== [Client {self.cid}] CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                features_class = np.stack(class_feature_map[class_label])\n",
    "                print(f\"== [Client {self.cid}] CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                metas_class = class_meta_map[class_label]\n",
    "                print(f\"== [Client {self.cid}] CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                k = centroids_dict[class_label].shape[0]\n",
    "                centroids = centroids_dict[class_label]\n",
    "                print(f\"\\n[Client {self.cid}]  Class {class_label}: {features_class.shape[0]} segments\")\n",
    "                print(f\"== [Client {self.cid}] CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                if features_class.shape[0] < k:\n",
    "                    print(f\"[Client {self.cid}] Skipping class {class_label} due to insufficient samples for clustering\")\n",
    "                    continue\n",
    "                print(f\"[client {self.cid}] start the Kmean-Clustering  \")\n",
    "                print(f\"== [Client {self.cid}] before Kmeans CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                kmeans = KMeans(n_clusters=k, init=centroids, n_init=1, max_iter=1, algorithm=\"elkan\", random_state=42)\n",
    "                kmeans.fit(features_class)\n",
    "                cluster_labels = kmeans.labels_\n",
    "                centers = kmeans.cluster_centers_\n",
    "                silhouette_scores[(classlabel, cluster_id)] = sil_score\n",
    "                #for meta, cid in zip(metas_class, cluster_labels):\n",
    "                #    meta['cluster_id'] = int(cid)                    \n",
    "                print(f\"== [Client {self.cid}] CUDA: After Kmeans allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "                print(f\"[client {self.cid}] Preparing for the evaluation function\")\n",
    "                counts = np.bincount(cluster_labels, minlength=k).astype(float)\n",
    "                cluster_list = []\n",
    "                for idx in range(k):\n",
    "                        cluster_list.append({\n",
    "                        \"centroid\": centers[idx].tolist(),\n",
    "                        \"count\": counts[idx]\n",
    "                        })\n",
    "                per_class_cluster_data[class_label] = cluster_list\n",
    "            centroids = []\n",
    "            counts = []\n",
    "            class_labels = []\n",
    "            cluster_indices = []\n",
    "            for class_label, cluster_list in per_class_cluster_data.items():\n",
    "                for cluster_idx, entry in enumerate(cluster_list):\n",
    "                    centroids.append(entry[\"centroid\"])     # Should already be list/np.ndarray\n",
    "                    counts.append(entry[\"count\"])\n",
    "                    class_labels.append(class_label)\n",
    "                    cluster_indices.append(cluster_idx)\n",
    "\n",
    "            centroids_array = np.array(centroids, dtype=np.float32)           # shape (N, D)\n",
    "            counts_array = np.array(counts, dtype=np.float32)                 # shape (N,)\n",
    "            class_labels_array = np.array(class_labels, dtype=np.int32)       # shape (N,)\n",
    "            cluster_indices_array = np.array(cluster_indices, dtype=np.int32) # shape (N,) \n",
    "            print(f\"[client {self.cid}] building up the metrics \") \n",
    "            print(f\"== [Client {self.cid}] after building the Per_class_cluster CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "            #metrics = {\"cluster_data\": json.dumps({int(k): v for k, v in per_class_cluster_data.items()})}\n",
    "            print(f\"== [Client {self.cid}] after building the metrics CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "            #dummy_array = np.zeros((1, 1), dtype=np.float32)\n",
    "            #self.save_state()\n",
    "            \n",
    "            tot_samples = sum([len(class_feature_map[c]) for c in per_class_cluster_data])\n",
    "            print(f\"== [Client {self.cid}] After saving the state and preparing the Eval cycle parameters CUDA: allocated {torch.cuda.memory_allocated() // (1024*1024)} MB, reserved {torch.cuda.memory_reserved() // (1024*1024)} MB\")\n",
    "            return [centroids_array, counts_array, class_labels_array, cluster_indices_array], tot_samples, {}  \n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            print(\"âŒ ERROR in client `fit()`:\")\n",
    "            traceback.print_exc()\n",
    "            raise e  # Let Flower fail visibly\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        #self.load_state()\n",
    "        print(f\"[Client {self.cid}] evaluate() called.\")\n",
    "        # Use current centroids from latest fit (already set by previous round)\n",
    "        if \"centroids\" not in config:\n",
    "            print(f\"[Client {self.cid}] No centroids provided in config for evaluation.\")\n",
    "            return 0.0, 0, {}  # Graceful exit with defaults\n",
    "\n",
    "        # Parse centroids from config\n",
    "        try:\n",
    "            centroids_dict = json.loads(config[\"centroids\"])\n",
    "            centroids_dict = {int(k): np.array(v, dtype=np.float32) for k, v in centroids_dict.items()}\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"[Client {self.cid}] Error parsing centroids: {e}\")\n",
    "            return 0.0, 0, {}  # Handle parsing errors gracefully\n",
    "\n",
    "        # Make sure we have features and cluster labels\n",
    "        if self.features is None :\n",
    "            print(f\"[Client {self.cid}] No features or cluster_labels available.\")\n",
    "            return 0.0, 0, {}\n",
    "\n",
    "        # Compute inertia: sum of squared distances to closest centroid for each point\n",
    "        from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "        class_inertia = {}\n",
    "        class_cluster_sizes = {}\n",
    "        total_samples = 0\n",
    "        class_feature_map = defaultdict(list)\n",
    "        class_meta_map = defaultdict(list)\n",
    "\n",
    "        for f, m in zip(self.features, self.metas):\n",
    "                    class_label = m['original_label']\n",
    "                    class_feature_map[class_label].append(f)\n",
    "                    class_meta_map[class_label].append(m)\n",
    "        for class_label in class_feature_map:\n",
    "            if class_label not in centroids_dict:\n",
    "                continue\n",
    "            features_class = np.stack(class_feature_map[class_label])\n",
    "            k = centroids_dict[class_label].shape[0]\n",
    "            centers = centroids_dict[class_label]\n",
    "            assignments = pairwise_distances_argmin(features_class, centers)\n",
    "            # Inertia: sum of squared distances from each sample to assigned centroid\n",
    "            inertia = np.sum((features_class - centers[assignments]) ** 2)\n",
    "            class_inertia[class_label] = float(inertia)\n",
    "            # Cluster sizes\n",
    "            sizes = np.bincount(assignments, minlength=k)\n",
    "            class_cluster_sizes[class_label] = {int(idx): int(sz) for idx, sz in enumerate(sizes)}\n",
    "            total_samples += len(features_class)\n",
    "        metrics = {\n",
    "            \"inertia\": float(inertia),\n",
    "            \"cluster_sizes\": json.dumps(class_cluster_sizes)\n",
    "        }\n",
    "        return float(sum(class_inertia.values())), total_samples, metrics\n",
    "\n",
    "\n",
    "    def build_clusters_by_class(self, final_clusters,final_info):\n",
    "            clusters_by_class = defaultdict(list)\n",
    "            for cluster, info in zip(final_clusters, final_info):\n",
    "            \n",
    "                if not cluster or \"original_label\" not in cluster[0]:\n",
    "                    print(\"[DEBUG] Skipping malformed or empty cluster\")\n",
    "                    continue\n",
    "\n",
    "                # Dynamically get the class ID from the first meta in the cluster\n",
    "                class_id = cluster[0][\"original_label\"]\n",
    "    \n",
    "                # Dynamically retrieve the class name from your mapping\n",
    "                class_name = self.label_to_classname[class_id]\n",
    "                cluster_id = info[\"cluster_id\"]\n",
    "                cluster_size = len(cluster)\n",
    "                # Sort cluster by CLIP score, highest first\n",
    "                sorted_cluster = sorted(cluster, key=lambda m: m[\"clip_score\"], reverse=True)\n",
    "                \n",
    "                # Optional: skip clusters where all scores are below threshold (e.g., 0.6)\n",
    "                #if all(m[\"clip_score\"] < 0.6 for m in sorted_cluster):\n",
    "                #    continue\n",
    "               # print(f\"[DEBUG Client {self.cid}]  Class '{class_name}' (ID {class_id}), Cluster {cluster_id}:\")\n",
    "               # print(f\"[DEBUG Client {self.cid}]         Size: {cluster_size}, Unique images: {unique_imgs}, Mean CLIP score: {mean_clip_score:.3f}\")\n",
    "               # print(f\"[DEBUG Client {self.cid}]         Info summary: {info}\")\n",
    "                # Append this sorted cluster and its info to the corresponding class grouping\n",
    "                clusters_by_class[class_name].append((sorted_cluster, {\"cluster_id\": cluster_id}))\n",
    "                print(f\"Accepted cluster: {class_name} | CLIP scores: {[m['clip_score'] for m in sorted_cluster]}\")\n",
    "\n",
    "            return clusters_by_class\n",
    "    \n",
    "############################### Visualization Helper functions ##########################\n",
    "    def plot_cluster_4x4(self, cluster, cluster_id, class_label):\n",
    "        # Ensure output directory exists\n",
    "        out_dir = \"cluster_visualizations\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        # Build an informative, safe filename\n",
    "        fname = f\"client_{self.cid}_class_{class_label}_cluster_{cluster_id}.png\"\n",
    "        save_path = os.path.join(out_dir, fname)\n",
    "        print(f\"Saving: {fname} | Class label: {class_label}, Cluster size: {len(cluster)}\")\n",
    "        fig = plt.figure(figsize=(12, 12))\n",
    "        plt.suptitle(f\"Class {class_label} | Cluster {cluster_id} | {len(cluster)} items\")\n",
    "        for i, meta in enumerate(cluster[:16]):\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            img = Image.open(meta['img_path']).convert(\"RGB\")\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(\n",
    "            f\"ImgID: {meta['original_image_index']}\\n\"\n",
    "            f\"Score: {meta['clip_score']:.2f}\\n\"\n",
    "            f\"Clip class: {meta['clip_class']}\", fontsize=8\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    def draw_arrow_on_image(self, ax, xy_from, xy_to, color='yellow', width=2, mutation_scale=15):\n",
    "        \"\"\"Draws an arrow from xy_from to xy_to on a matplotlib axis.\"\"\"\n",
    "        ax.annotate(\n",
    "            '', xy=xy_to, xytext=xy_from,\n",
    "            arrowprops=dict(\n",
    "                arrowstyle='->',\n",
    "                color=color,\n",
    "                lw=width,\n",
    "                shrinkA=0,\n",
    "                shrinkB=0,\n",
    "                mutation_scale=mutation_scale\n",
    "            ),\n",
    "            annotation_clip=False\n",
    "        )\n",
    "\n",
    "    def create_mask_overlay_from_original(self, original_img_path, mask_img, alpha=0.6, mask_color=(255, 180, 0)):\n",
    "        \"\"\" Overlay a binary mask onto the original full image.\"\"\"\n",
    "        import cv2  # Ensure cv2 is imported at top if not\n",
    "        orig = np.array(Image.open(original_img_path).convert(\"RGB\")).copy()\n",
    "        mask = np.array(mask_img)\n",
    "        if mask.max() > 1:\n",
    "           mask_bin = (mask[..., 0] > 0).astype(np.uint8)\n",
    "        else:\n",
    "            mask_bin = mask[..., 0]\n",
    "        mask_rgb = np.zeros_like(orig)\n",
    "        mask_rgb[mask_bin == 1] = mask_color\n",
    "        out = cv2.addWeighted(mask_rgb, alpha, orig, 1 - alpha, 0)\n",
    "        return Image.fromarray(out)\n",
    "\n",
    "\n",
    "    def visualize_clusters_with_overlay_and_arrow(self, clusters_by_class, max_per_cluster=4):\n",
    "        \"\"\" Visualizes each cluster with segment (left) and overlay on original image (right), with arrow.\"\"\"\n",
    "        for class_name, cluster_info_list in clusters_by_class.items():\n",
    "            n_clusters = len(cluster_info_list)\n",
    "            fig, axes = plt.subplots(n_clusters, max_per_cluster, figsize=(5 * max_per_cluster, 4 * n_clusters))\n",
    "            if n_clusters == 1:\n",
    "                axes = np.expand_dims(axes, 0)\n",
    "            plt.suptitle(f\"Class: {class_name} Clusters (sorted by CLIP saliency)\", fontsize=16)\n",
    "\n",
    "            for r, (cluster, info) in enumerate(cluster_info_list):\n",
    "                for c in range(max_per_cluster):\n",
    "                    ax = axes[r, c] if n_clusters > 1 else axes[0, c]\n",
    "                    try:\n",
    "                        meta = cluster[c]\n",
    "                    except IndexError:\n",
    "                        ax.axis('off')\n",
    "                        continue\n",
    "\n",
    "                # Load the segment image as saved (RGB)\n",
    "                    seg_img = Image.open(meta['img_path']).convert(\"RGB\")\n",
    "                    seg_np = np.array(seg_img)\n",
    "\n",
    "                # Derive binary mask (used only for overlay)\n",
    "                    import cv2\n",
    "                    gray = cv2.cvtColor(seg_np, cv2.COLOR_RGB2GRAY)\n",
    "                    mask_only = np.where(gray > 5, 255, 0).astype(np.uint8)\n",
    "                    mask_only = np.expand_dims(mask_only, axis=-1)\n",
    "\n",
    "                # Load the original image and create overlay\n",
    "                    orig_img_path = meta.get('original_image_path')  # Assume this field exists in meta\n",
    "                    overlay_img = self.create_mask_overlay_from_original(orig_img_path, mask_only, alpha=0.6)\n",
    "\n",
    "                # Resize both views\n",
    "                    thumb_segment = seg_img.resize((64, 64), Image.LANCZOS)\n",
    "                    thumb_overlay = overlay_img.resize((128, 128), Image.LANCZOS)\n",
    "\n",
    "                # Compose side-by-side image\n",
    "                    combined = Image.new('RGB', (thumb_segment.width + thumb_overlay.width, thumb_overlay.height), (255, 255, 255))\n",
    "                    combined.paste(thumb_segment, (0, 0))\n",
    "                    combined.paste(thumb_overlay, (thumb_segment.width, 0))\n",
    "\n",
    "                # Show in subplot\n",
    "                    ax.imshow(combined)\n",
    "                    ax.axis('off')\n",
    "\n",
    "                # Draw arrow between segment and overlay\n",
    "                    arrow_start = (thumb_segment.width // 2, thumb_overlay.height // 2)\n",
    "                    arrow_end = (thumb_segment.width + thumb_overlay.width // 2, thumb_overlay.height // 2)\n",
    "                    self.draw_arrow_on_image(ax, arrow_start, arrow_end)\n",
    "\n",
    "                # Add title with cluster info\n",
    "                    ax.set_title(f\"CID: {info['cluster_id']} | Score: {meta['clip_score']:.2f} \\n Clip Interpretation: {meta['clip_class']}\", fontsize=9)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
    "            plt.savefig(f\"cluster_visualizations/[Client_{self.cid}]Class{class_name}_cluster_segment_overlay_images.png\")\n",
    "            plt.close(fig)\n",
    "    def visualize_clusters_umap_3d(self, structured_clusters):\n",
    "        os.makedirs(\"cluster_visualizations\", exist_ok=True)\n",
    "        for class_label, clusters in structured_clusters.items():\n",
    "            all_feats = []\n",
    "            all_labels = []\n",
    "            for cluster, info in clusters:\n",
    "                cluster_feats = [f for f in self.extract_features_dinov2_from_metas(cluster)]\n",
    "                cluster_id = info['cluster_id']\n",
    "                all_feats.extend(cluster_feats)\n",
    "                all_labels.extend([cluster_id] * len(cluster_feats))\n",
    "\n",
    "            if len(set(all_labels)) < 2:\n",
    "                continue\n",
    "\n",
    "            reducer = umap.UMAP(n_components=3, random_state=42)\n",
    "            reduced = reducer.fit_transform(np.array(all_feats))\n",
    "   \n",
    "            all_labels = np.array(all_labels)\n",
    "            fig = plt.figure(figsize=(10, 8))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            colors = cm.get_cmap('tab10', np.max(all_labels)+1)\n",
    "            for cluster_id in np.unique(all_labels):\n",
    "                idx = all_labels == cluster_id\n",
    "                ax.scatter(reduced[idx, 0], reduced[idx, 1], reduced[idx, 2],\n",
    "                           label=f\"Cluster {cluster_id}\", alpha=0.6, s=30,\n",
    "                           color=colors(cluster_id))\n",
    "            ax.set_title(f\"[Client {self.cid}] 3D UMAP of Class '{class_label}' by Cluster\")\n",
    "            ax.legend()\n",
    "            ax.view_init(elev=20, azim=135)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"cluster_visualizations/[Client]_{self.cid}UMAP3D_{class_label}.png\")\n",
    "            plt.close(fig)\n",
    "\n",
    "    def extract_features_dinov2_from_metas(self, metas, batch_size=256):\n",
    "        img_tensors = []\n",
    "        feats_accum = []\n",
    "        for i, meta in enumerate(metas):\n",
    "            img = Image.open(meta['img_path']).convert(\"RGB\")\n",
    "            img_tensor = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "            ])(img)\n",
    "            img_tensors.append(img_tensor)\n",
    "            if len(img_tensors) == batch_size or (i == len(metas) - 1):\n",
    "                batch = torch.stack(img_tensors).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                        feats = self.dinov2(batch).cpu().numpy()\n",
    "                feats_accum.extend(feats)\n",
    "                del batch\n",
    "                torch.cuda.empty_cache()\n",
    "                img_tensors = []\n",
    "        return feats_accum        \n",
    "        #if not img_tensors:\n",
    "#########################################################################################\n",
    "    def train_cavs_and_tcav(self):\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        inception_model = inception_v3(weights='IMAGENET1K_V1').to(device)\n",
    "        inception_model.eval()\n",
    "\n",
    "        inception_features = {}\n",
    "        gradients = {}\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            inception_features['mixed_8'] = output.detach()\n",
    "\n",
    "        def save_gradient(module, grad_input, grad_output):\n",
    "            gradients['mixed_8'] = grad_output[0]\n",
    "\n",
    "        inception_model.Mixed_7b.register_forward_hook(hook_fn)\n",
    "        inception_model.Mixed_7b.register_backward_hook(save_gradient)\n",
    "\n",
    "        inception_preproc = transforms.Compose([\n",
    "            transforms.Resize((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        def extract_mixed8_features(segment_metas):\n",
    "            images = []\n",
    "            feats = []\n",
    "            for meta in segment_metas:\n",
    "                img_path = meta['img_path']\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = inception_preproc(img)\n",
    "                images.append(img_tensor)\n",
    "            if not images:\n",
    "                return np.zeros((0, 1280))\n",
    "            batch_size = 512\n",
    "            for i in range(0, len(images), batch_size):\n",
    "                batch=torch.stack(images[i:i + batch_size]).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    _ = inception_model(batch)\n",
    "                    batch_feats = inception_features['mixed_8'].mean(dim=[2, 3])\n",
    "                feats.append(batch_feats.cpu().numpy())    \n",
    "                torch.cuda.empty_cache()\n",
    "            return np.concatenate(feats, axis=0)\n",
    "\n",
    "        def calculate_cav(concept_features, random_features):\n",
    "            X = np.concatenate([concept_features, random_features], axis=0)\n",
    "            y = np.array([1]*len(concept_features) + [0]*len(random_features))\n",
    "            clf = LogisticRegression(max_iter=1000)\n",
    "            clf.fit(X, y)\n",
    "            cav = clf.coef_[0]\n",
    "            cav = cav / np.linalg.norm(cav)\n",
    "            mean_pos = np.mean(np.dot(concept_features, cav))\n",
    "            mean_neg = np.mean(np.dot(random_features, cav))\n",
    "            if mean_pos < mean_neg:\n",
    "                cav = -cav\n",
    "            return cav \n",
    "\n",
    "        def get_mixed8_gradients(img, target_class_idx):\n",
    "            inception_features.clear()\n",
    "            gradients.clear()\n",
    "            tensor = inception_preproc(img).unsqueeze(0).to(self.device)\n",
    "            tensor.requires_grad_(True)\n",
    "            output = inception_model(tensor)\n",
    "            target_logit = output[:, target_class_idx]\n",
    "            inception_model.zero_grad()\n",
    "            target_logit.backward()\n",
    "            grad = gradients['mixed_8'].detach().cpu().numpy().squeeze()\n",
    "            grad_pooled = grad.mean(axis=(1,2))\n",
    "            return grad_pooled\n",
    "\n",
    "        def compute_tcav_score_for_class(class_images, cav, target_class_idx):\n",
    "            pos = 0\n",
    "            for img in class_images:\n",
    "                grad_pooled = get_mixed8_gradients(img, target_class_idx)\n",
    "                grad_pooled = grad_pooled / np.linalg.norm(grad_pooled)\n",
    "                if np.dot(grad_pooled, cav) > 0:\n",
    "                    pos += 1\n",
    "            return pos / len(class_images)\n",
    "\n",
    "        def load_test_images_by_class(test_dir):\n",
    "            class_images = {}\n",
    "            for class_label in os.listdir(test_dir):\n",
    "                if class_label.startswith('.'):\n",
    "                    continue\n",
    "                class_path = os.path.join(test_dir, class_label)\n",
    "                if not os.path.isdir(class_path):\n",
    "                    continue\n",
    "                imgs = []\n",
    "                for fname in os.listdir(class_path):\n",
    "                    if fname.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "                        img = Image.open(os.path.join(class_path, fname)).convert('RGB')\n",
    "                        imgs.append(img)\n",
    "                class_images[class_label] = imgs\n",
    "            return class_images\n",
    "\n",
    "        def plot_tcav_heatmap(tcav_matrix, class_labels, cluster_labels, client_id, out_dir=\"cluster_visualizations\", concept_class=\"???\"):\n",
    "            \"\"\"\n",
    "            tcav_matrix: np.ndarray of shape (num_clusters, num_classes)\n",
    "            class_labels: list of class names (columns)\n",
    "            cluster_labels: list of cluster names/ids (rows)\n",
    "            client_id: client numeric id\n",
    "            out_dir: directory to save the output image\n",
    "            \"\"\"\n",
    "            import matplotlib\n",
    "            import matplotlib.pyplot as plt\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            fig, ax = plt.subplots(figsize=(1.2 * len(class_labels) + 2, 0.8 * len(cluster_labels) + 3))\n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list('red_green', ['red', 'yellow', 'green'])\n",
    "            cax = ax.imshow(tcav_matrix, cmap=cmap, vmin=0., vmax=1.)\n",
    "\n",
    "            for i in range(tcav_matrix.shape[0]):\n",
    "                for j in range(tcav_matrix.shape[1]):\n",
    "                    val = tcav_matrix[i, j]\n",
    "                    ax.text(j, i, f\"{val:.2f}\", va='center', ha='center',\n",
    "                            color=\"black\" if val < 0.5 else \"white\", fontsize=10, fontweight='bold')\n",
    "            ax.set_xticks(np.arange(len(class_labels)))\n",
    "            ax.set_yticks(np.arange(len(cluster_labels)))\n",
    "            ax.set_xticklabels(class_labels, rotation=45, ha=\"right\", fontsize=12)\n",
    "            ax.set_yticklabels(cluster_labels, fontsize=12)\n",
    "            ax.set_xlabel(\"Class label\")\n",
    "            ax.set_ylabel(\"Concept (Cluster)\")\n",
    "            plt.title(f\"TCAV class {concept_class} Concepts Contribution\\nClient {client_id}\", fontsize=15, pad=25)\n",
    "            plt.colorbar(cax, ax=ax, fraction=0.045)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(out_dir, f\"TCAV_heatmap_class_{concept_class}_client{client_id}.png\"))\n",
    "            plt.close(fig)\n",
    "\n",
    "        final_clusters = []\n",
    "        final_info = []\n",
    "        per_class_clusters = defaultdict(lambda: defaultdict(list))\n",
    "        for meta in self.metas:\n",
    "            class_label = meta['original_label']\n",
    "            cluster_id = meta['cluster_id']\n",
    "            per_class_clusters[class_label][cluster_id].append(meta)\n",
    "\n",
    "        for class_label, cluster_dict in per_class_clusters.items():\n",
    "            metas_class = []\n",
    "            for cluster in cluster_dict.values():\n",
    "                metas_class.extend(cluster)\n",
    "            num_discovery_images = len(set(m['original_image_index'] for m in metas_class))\n",
    "            print(f\"{num_discovery_images} unique discovery images for class {class_label}\")\n",
    "\n",
    "            for c_id, cluster in cluster_dict.items():\n",
    "                img_indices = [m['original_image_index'] for m in cluster]\n",
    "                unique_imgs = set(img_indices)\n",
    "                cluster_size = len(cluster)\n",
    "                num_unique_imgs = len(unique_imgs)\n",
    "                clip_scores = [m['clip_score'] for m in cluster]\n",
    "                mean_clip_score = np.mean(clip_scores) if clip_scores else 0\n",
    "                keep = num_unique_imgs >= 10 or (cluster_size >= 30 and mean_clip_score > 0.75)\n",
    "                print(\n",
    "            f\"Class {class_label} Cluster {c_id}: \"\n",
    "            f\"size={cluster_size}, unique_imgs={num_unique_imgs}, \"\n",
    "            f\"mean_clip_score={mean_clip_score:.3f}, keep={keep}\")\n",
    "                for meta in cluster:\n",
    "                    meta[\"cluster_id\"] = c_id\n",
    "                final_clusters.append(cluster)\n",
    "                final_info.append({\n",
    "                \"class_label\": class_label,\n",
    "                \"cluster_id\": c_id,\n",
    "                \"num_members\": cluster_size,\n",
    "                \"num_images\": num_unique_imgs,\n",
    "                \"mean_clip_score\": mean_clip_score\n",
    "                })\n",
    "        \n",
    "        clusters_by_class = self.build_clusters_by_class(final_clusters,final_info)\n",
    "        for class_label, clusters in clusters_by_class.items():\n",
    "            for cid, (cluster_metas, info) in enumerate(clusters):\n",
    "                self.plot_cluster_4x4(cluster_metas, cid, class_label)\n",
    "        self.visualize_clusters_with_overlay_and_arrow(clusters_by_class, max_per_cluster=4)\n",
    "        # 3D UMAP Visualization per class\n",
    "        self.visualize_clusters_umap_3d(clusters_by_class)\n",
    "\n",
    "        # Construct cluster CAVs\n",
    "        cluster_cavs = {}\n",
    "        for class_label, clusters in clusters_by_class.items():\n",
    "            for cluster, info in clusters:\n",
    "                cid = info['cluster_id']\n",
    "                other_same_class = [m for c, i in clusters if i['cluster_id'] != cid for m in c]\n",
    "                other_classes = [m for l, cls in clusters_by_class.items() if l != class_label for c, _ in cls for m in c]\n",
    "                n_pos = len(cluster)\n",
    "                n_same = min(n_pos // 2, len(other_same_class))\n",
    "                n_other = min(n_pos - n_same, len(other_classes))\n",
    "                if n_same + n_other == 0:\n",
    "                    continue\n",
    "                neg_samples = random.sample(other_same_class, n_same) + random.sample(other_classes, n_other)\n",
    "                pos_feats = extract_mixed8_features(cluster)\n",
    "                neg_feats = extract_mixed8_features(neg_samples)\n",
    "                cav = calculate_cav(pos_feats, neg_feats)\n",
    "                cluster_cavs[(class_label, cid)] = cav\n",
    "                #### --------   I need to show a statment of the constructed CAV --------####\n",
    "            print(f\"all the related CAV of Class {class_label} was successfully generated : \\n {cluster_cavs} \\n\")\n",
    "        test_dir = '/gpfs/helios/home/mahmouds/Thesis/Test'\n",
    "        image_dir= '/gpfs/helios/home/mahmouds/Thesis/data/ILSVRC2012/segmentation'\n",
    "        class_to_inception_idx = {\n",
    "            'basketball': 816,\n",
    "            #'corn ears': 499,\n",
    "            #'electric ray': 445,\n",
    "            'mountain bike': 162,\n",
    "            'moving van': 191\n",
    "            #'soccer ball': 817\n",
    "        }\n",
    "        test_images = load_test_images_by_class(test_dir)\n",
    "\n",
    "        #for (class_label, cid), cav in cluster_cavs.items():\n",
    "        #    if class_label not in test_images or class_label not in class_to_inception_idx:\n",
    "        #        continue\n",
    "        #    score = compute_tcav_score_for_class(test_images[class_label], cav, class_to_inception_idx[class_label])\n",
    "        #    print(f\" [Client {self.cid}] TCAV score for class {class_label} cluster {cid}: {score:.2f}\")\n",
    "        #    #### ---- I need to report properly the test image with the contributed clusters ------####\n",
    "        # ----- Build list of all clusters and class labels for the matrix dimensions\n",
    "        clusters = list(cluster_cavs.keys())\n",
    "        cluster_labels = [f\"{self.label_to_classname.get(cl, cl)}-C{cid}\" for cl, cid in clusters]\n",
    "        class_labels = sorted(test_images.keys())\n",
    "\n",
    "        # Initialize the matrix\n",
    "        tcav_matrix = np.zeros((len(clusters), len(class_labels)))\n",
    "\n",
    "# Fill matrix: rows=clusters, cols=class labels\n",
    "        for i, (class_label, cid) in enumerate(clusters):\n",
    "            for j, test_class in enumerate(class_labels):\n",
    "                if test_class not in test_images or class_label not in class_to_inception_idx:\n",
    "                    tcav_matrix[i, j] = np.nan  # gray if missing\n",
    "                    continue\n",
    "                score = compute_tcav_score_for_class(\n",
    "                    test_images[test_class],\n",
    "                    cluster_cavs[(class_label, cid)],\n",
    "                    class_to_inception_idx[test_class]\n",
    "                )\n",
    "                tcav_matrix[i, j] = score\n",
    "        for concept_class in unique_classes:\n",
    "            rows = [i for i, (cl, _) in enumerate(clusters) if cl == concept_class]\n",
    "            if not rows:\n",
    "                continue  # No clusters for this class, skip\n",
    "            submatrix = tcav_matrix[rows, :]\n",
    "            sub_cluster_labels = [cluster_labels[i] for i in rows]\n",
    "            \n",
    "        # After the loop, immediately call:\n",
    "            #plot_tcav_heatmap(tcav_matrix, class_labels, cluster_labels, client_id=self.cid)\n",
    "            plot_tcav_heatmap(submatrix, class_labels, sub_cluster_labels, client_id=self.cid, out_dir=\"cluster_visualizations\",concept_class=concept_class)\n",
    "            print(f\"[Client {self.cid}] Saved TCAV class{concept_class} concepts heatmap to cluster_visualizations/TCAV_heatmap_client{self.cid}.png\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be005451-bf8e-4f20-91b3-b457fb5da81b",
   "metadata": {},
   "source": [
    "### Main funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec17a738-8c54-4d1e-a771-53b4cae948b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/space/software/cluster_software/spack/linux-rhel9-x86_64/gcc-9.2.0/python-3.12.3-jfsvfhxwrihwyloazioovxwqtuwcg6qf/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=2244764) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!export FLWR_LOG_LEVEL=DEBUG\n",
    "!export GRPC_VERBOSITY=DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e14a14d-a998-4915-9ffd-98bf8e4d8b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /gpfs/helios/home/mahmouds/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed 15 centroids for class 0\n",
      "Computed 15 centroids for class 1\n",
      "Computed 15 centroids for class 2\n",
      "\n",
      "Total classes initialized: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /gpfs/helios/home/mahmouds/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Starting SAM2 segmentation and saving segments with metadata for client 0 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2168 [00:00<?, ?it/s]/gpfs/helios/home/mahmouds/sam2/sam2/sam2_image_predictor.py:431: UserWarning: cannot import name '_C' from 'sam2' (/gpfs/helios/home/mahmouds/sam2/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  masks = self._transforms.postprocess_masks(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [26:25<00:00,  1.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 0] Segmentation done. Starting feature extraction...\n",
      "{'total_attempted': 24939, 'clip_filtered_out': 15445, 'clip_passed': 9494}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [01:35<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 0] Extracted features: torch.Size([9494, 384]), Labels: 9494\n",
      "ðŸ”§ Starting SAM2 segmentation and saving segments with metadata for client 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [29:18<00:00,  1.23it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 1] Segmentation done. Starting feature extraction...\n",
      "{'total_attempted': 26156, 'clip_filtered_out': 16123, 'clip_passed': 10033}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:57<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 1] Extracted features: torch.Size([10033, 384]), Labels: 10033\n",
      "ðŸ”§ Starting SAM2 segmentation and saving segments with metadata for client 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2168/2168 [27:47<00:00,  1.30it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 2] Segmentation done. Starting feature extraction...\n",
      "{'total_attempted': 26254, 'clip_filtered_out': 16284, 'clip_passed': 9970}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting DINOv2 features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:44<00:00,  5.24s/it]\n",
      "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
      "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
      "\n",
      "\t\t$ flwr new  # Create a new Flower app from a template\n",
      "\n",
      "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
      "\n",
      "\tUsing `start_simulation()` is deprecated.\n",
      "\n",
      "            This is a deprecated feature. It will be removed\n",
      "            entirely in future versions of Flower.\n",
      "        \n",
      "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=50, no round_timeout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Client 2] Extracted features: torch.Size([9970, 384]), Labels: 9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 21:21:49,619\tINFO worker.py:1771 -- Started a local Ray instance.\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:A100': 1.0, 'node:__internal_head__': 1.0, 'node:172.16.10.127': 1.0, 'CPU': 192.0, 'memory': 480416889856.0, 'object_store_memory': 5368709120.0, 'GPU': 1.0}\n",
      "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 4, 'num_gpus': 0.25}\n",
      "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
      "\u001b[92mINFO \u001b[0m:      [INIT]\n",
      "\u001b[92mINFO \u001b[0m:      Using initial global parameters provided by strategy\n",
      "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
      "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.0, {}\n",
      "\u001b[92mINFO \u001b[0m:      \n",
      "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
      "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Global evaluation for round 0 skipped (no global model).\n",
      "[Strategy] configure_fit called for round 1\n",
      "[Strategy] Available clients: 3\n"
     ]
    }
   ],
   "source": [
    "#! export RAY_LOG_TO_STDERR=1\n",
    "#logging.basicConfig(level=logging.INFO)\n",
    "#logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load CLIP model\n",
    "\n",
    "\n",
    "def precompute_client_data(num_clients=3):\n",
    "    import pickle\n",
    "    import logging\n",
    "    #logging.getLogger('root').setLevel(logging.WARNING)\n",
    "    root_dir = '/gpfs/helios/home/mahmouds/Thesis/data/ILSVRC2012/segmentation'\n",
    "    SAM_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    dino_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    imagenet_dataset = datasets.ImageFolder(root=root_dir, transform=SAM_transform)\n",
    "    \n",
    "   \n",
    "\n",
    "        # Load SAM2 model\n",
    "    sam2_checkpoint = \"/gpfs/helios/home/mahmouds/sam2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "    model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "    assert os.path.exists(sam2_checkpoint), f\"SAM2 checkpoint {sam2_checkpoint} not found.\"\n",
    "\n",
    "    sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "    dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device).eval() #ViT-B/32\n",
    "    mask_gen = SAM2AutomaticMaskGenerator(\n",
    "            model=sam2,\n",
    "            predicted_iou_thresh=0.88,\n",
    "            stability_score_thresh=0.8,\n",
    "            box_nms_thresh=0.5,\n",
    "            crop_n_layers=0,\n",
    "            min_mask_region_area=256,\n",
    "            max_num_masks=30\n",
    "        )\n",
    "\n",
    "        # Dataset and segmentation\n",
    "    SAM_transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    imagenet_dataset = datasets.ImageFolder(root=root_dir, transform=SAM_transform)\n",
    "    for cid in range(num_clients):\n",
    "        print(f\"ðŸ”§ Starting SAM2 segmentation and saving segments with metadata for client {cid} ...\")\n",
    "        #logger.info(f\"Precomputing for client {cid}...\")\n",
    "        segment_root = f'segment_dataset/segments/segments_client_{cid}'\n",
    "        os.makedirs(segment_root, exist_ok=True)\n",
    "        #client_features, client_labels, client_metas = [], [], []\n",
    "        #for img_idx, (image_tensor, label) in tqdm(enumerate(imagenet_dataset), total=len(imagenet_dataset)):\n",
    "        #    if img_idx % num_clients != cid:\n",
    "        #        continue\n",
    "\n",
    "        #    orig_img_path, _ = imagenet_dataset.samples[img_idx]\n",
    "        #    img_pil = Image.open(orig_img_path).convert(\"RGB\")\n",
    "        #    img_np = np.array(img_pil)\n",
    "            \n",
    "        #    try:\n",
    "        #        masks = mask_gen.generate(img_np)\n",
    "        #        if not masks or len(masks) == 0:\n",
    "        #            print(f\"[Client {cid}] âš ï¸ No masks returned for image {img_idx}: {orig_img_path}\")\n",
    "        #            continue    \n",
    "        #    except Exception as e:\n",
    "        #        print(f\"[Client {cid}] Segmentation failed on image {orig_img_path}: {e}\")\n",
    "        #        continue\n",
    "\n",
    "        #    segments, scores, bboxes = [], [], []\n",
    "        #    for m in masks:\n",
    "        #        mask = m[\"segmentation\"].astype(bool)\n",
    "        #        seg = img_np.copy()\n",
    "        #        seg[~mask] = 0\n",
    "        #        segments.append(seg)\n",
    "        #        scores.append(m[\"predicted_iou\"])\n",
    "        #        bboxes.append(get_bbox_from_mask(mask))\n",
    "\n",
    "        #    class_name = imagenet_dataset.classes[label]\n",
    "        #    class_dir = os.path.join(segment_root, class_name)\n",
    "\n",
    "        #    save_segments_with_metadata(\n",
    "        #        img_idx, label, segments, scores, bboxes, orig_img_path, class_dir, score_threshold=0.65\n",
    "        #    )\n",
    "\n",
    "        print(f\"[Client {cid}] Segmentation done. Starting feature extraction...\")\n",
    "            \n",
    "        \n",
    "        #segment_dataset = SegmentDataset(segment_root=segment_root, transform=dino_transform)\n",
    "        #print(segment_dataset.get_stats())\n",
    "        #segment_loader = DataLoader(segment_dataset, batch_size=512, shuffle=False, collate_fn=custom_collate)\n",
    "        \n",
    "        #features, labels, metas = extract_features_dinov2(segment_loader, dinov2_model, device)\n",
    "        #print(f\"[Client {cid}] Extracted features: {features.shape}, Labels: {len(labels)}\")\n",
    "        #data = {'features': features, 'labels': labels, 'metas': metas}\n",
    "        #with open(f'segment_dataset/segments/precomputed_client_{cid}.pkl', 'wb') as f:\n",
    "        #        pickle.dump(data, f)\n",
    "        logging.info(f\"Precomputed data saved for client {cid}.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device).eval()\n",
    "    number_of_clusters =15\n",
    "    num_clients = 3\n",
    "    np.random.seed(42)\n",
    "\n",
    "    root_dir_init = '/gpfs/helios/home/mahmouds/Thesis/root/'\n",
    "    def dino_transform():\n",
    "        return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    transform_init = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    dataset_init = datasets.ImageFolder(root=root_dir_init, transform=transform_init)\n",
    "    dino_transform = dino_transform()\n",
    "    \n",
    "    class_to_img_path = defaultdict(list)\n",
    "    for img_idx, (img_tensor, label) in enumerate(dataset_init):\n",
    "        img_path, _ = dataset_init.samples[img_idx]\n",
    "        class_to_img_path[label].append(img_path)\n",
    "    \n",
    "    initial_centroids_per_class = {}\n",
    "    sampled_imgs = []\n",
    "    class_labels = []\n",
    "     #Compute all DINOv2 features per class\n",
    "    initial_centroids_per_class = {}\n",
    "    for class_label, paths in class_to_img_path.items():\n",
    "        img_tensors = []\n",
    "        for img_path in paths:\n",
    "            img_pil = Image.open(img_path).convert(\"RGB\")\n",
    "            img_tensors.append(dino_transform(img_pil))\n",
    "        batch = torch.stack(img_tensors).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = dinov2_model(batch).cpu().numpy()\n",
    "        # Get KMeans++ initial centroids for this class\n",
    "        kmeans = KMeans(n_clusters=number_of_clusters, init='k-means++', random_state=42)\n",
    "        kmeans.fit(features)\n",
    "        initial_centroids_per_class[class_label] = kmeans.cluster_centers_  # shape: (K, D)\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Computed {number_of_clusters} centroids for class {class_label}\")\n",
    "\n",
    "    print(f\"\\nTotal classes initialized: {len(initial_centroids_per_class)}\")\n",
    "    strategy = FedKMeansStrategy(initial_centroids=initial_centroids_per_class, num_clusters=number_of_clusters, tol=.5)\n",
    "    precompute_client_data(num_clients)\n",
    "    \n",
    "    def client_fn(context):\n",
    "     import pickle\n",
    "     cid = int(context.node_config['partition-id'])\n",
    "     print(f\"[Client {cid}] Starting initialization...\")\n",
    "\n",
    "     try:\n",
    "        print(f\"[Client {cid}] Attempting to load pickle file...\") \n",
    "        with open(f'segment_dataset/segments/precomputed_client_{cid}.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(f\"[Client {cid}] Pickle file loaded successfully\")\n",
    "        features = data['features']\n",
    "        labels = data['labels']\n",
    "        metas = data['metas']\n",
    "        print(f\"[Client {cid}] Data extracted: features={features.shape}, labels={len(labels)}, metas={len(metas)}\")\n",
    "        client = ThesisKMeansClient(cid,features, labels, metas, device)\n",
    "        print(f\"[Client {cid}] ThesisKMeansClient created successfully\")\n",
    "        return client.to_client()\n",
    "     except Exception as e:\n",
    "        print(f\"[Client {self.cid}] failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "    fl.simulation.start_simulation(\n",
    "        client_fn=client_fn,\n",
    "        num_clients=num_clients,\n",
    "        config=fl.server.ServerConfig(num_rounds=50),\n",
    "        strategy=strategy,\n",
    "        client_resources={\"num_cpus\": 4, \"num_gpus\": 0.25},\n",
    "        ray_init_args={\"object_store_memory\": 5 * 1024 * 1024 * 1024,  # 5GiB\n",
    "            \"runtime_env\": {\"pip\": [\n",
    "    \"torch\", \"torchvision\", \"flwr\", \"xformers\", \"umap-learn\", \n",
    "            \"kneed\", \"scikit-learn\", \"opencv-python-headless\", \"clip\", \n",
    "            \"ftfy\", \"regex\", \"tqdm\", \"Pillow\"\n",
    "]}}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68be64d-fe7c-4ead-8a8f-ea08b50bda26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis3.12",
   "language": "python",
   "name": "thesis3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
