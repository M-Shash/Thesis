{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af513d1-f9b7-416c-ab52-be170b85e545",
   "metadata": {},
   "source": [
    "**Library Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85fb77ee-ecd4-41bf-81a3-91a0f94f3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import uuid\n",
    "import warnings\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from kneed import KneeLocator\n",
    "from flwr.common import Context, ndarrays_to_parameters, FitIns, EvaluateIns\n",
    "from flwr.client import NumPyClient\n",
    "from flwr.server.strategy import FedAvg\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d40752-fb2b-4567-a2ca-8572f9c1d33c",
   "metadata": {},
   "source": [
    "**Utility Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66168e5b-2481-4f3b-80a3-c3f4f895f2a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Dataset setup\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "imagenet_subset = ImageFolder(root='/gpfs/helios/home/mahmouds/Thesis/data/ILSVRC2012', transform=transform)\n",
    "\n",
    "# Dirichlet sampling\n",
    "def create_dirichlet_clients(dataset, num_clients=3, alpha=1, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    num_classes = len(dataset.classes)\n",
    "    data_indices = [[] for _ in range(num_classes)]\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        data_indices[label].append(idx)\n",
    "    client_indices = defaultdict(list)\n",
    "    for c in range(num_classes):\n",
    "        idxs = np.array(data_indices[c])\n",
    "        np.random.shuffle(idxs)\n",
    "        proportions = np.random.dirichlet(alpha * np.ones(num_clients))\n",
    "        proportions = (np.cumsum(proportions) * len(idxs)).astype(int)[:-1]\n",
    "        split = np.split(idxs, proportions)\n",
    "        for i, chunk in enumerate(split):\n",
    "            client_indices[i].extend(chunk.tolist())\n",
    "    return client_indices\n",
    "\n",
    "\n",
    "\n",
    "# Visualization\n",
    "def plot_real_client_distributions( client_indices, num_clients=3):\n",
    "    try:\n",
    "        dataset=imagenet_subset\n",
    "        #print (f\"the Dataset is{imagenet_subset.classes}  \")\n",
    "        #print (f\"And the client Indices are {client_indices}\")\n",
    "        idx_to_class = {i: name for i, name in enumerate(dataset.classes)}\n",
    "        num_classes = len(idx_to_class)\n",
    "        class_counts = np.zeros((num_clients, num_classes), dtype=int)\n",
    "        for client_id, indices in client_indices.items():\n",
    "            labels = [dataset[idx][1] for idx in indices]\n",
    "            for label in labels:\n",
    "                class_counts[client_id, label] += 1\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        bottom = np.zeros(num_clients)\n",
    "        for cls in range(num_classes):\n",
    "            heights = class_counts[:, cls]\n",
    "            total_per_client = class_counts.sum(axis=1)\n",
    "            proportions = heights / total_per_client\n",
    "            bars = ax.bar(range(num_clients), proportions, bottom=bottom, label=idx_to_class[cls])\n",
    "            for i, bar in enumerate(bars):\n",
    "                if heights[i] > 0:\n",
    "                    ax.text(\n",
    "                        bar.get_x() + bar.get_width() / 2,\n",
    "                        bottom[i] + proportions[i] / 2,\n",
    "                        str(heights[i]),\n",
    "                        ha='center', va='center', fontsize=8, color='white'\n",
    "                    )\n",
    "            bottom += proportions\n",
    "        ax.set_title('Client-Class Distribution (Proportion + Raw Count)')\n",
    "        ax.set_xlabel('Client ID')\n",
    "        ax.set_ylabel('Proportion of Samples')\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.legend(ncol=2, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.cid} Data Distribution.png\")\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "            print(f\"Client can't plot the data distribution because of : {e}\")\n",
    "        \n",
    "\n",
    "def unnormalize_image(tensor_img):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = tensor_img * std + mean\n",
    "    return torch.clamp(img, 0, 1)\n",
    "\n",
    "def visualize_clusters(client_clusters, dataset_classes, sample_per_class=500,client=\"client_0\", top_k_images=5):\n",
    "    for class_id, data in client_clusters.items():\n",
    "        class_name = dataset_classes[class_id]\n",
    "        #truncate_n = min(sample_per_class, len(data['features_raw']))\n",
    "        features = np.array(data['features_raw'])#[:truncate_n]\n",
    "        cluster_ids = np.array(data['cluster_ids'])#[:truncate_n]\n",
    "        imgs = data['images']#[:truncate_n]\n",
    "        features_2d = data['features_2d']\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios': [4, 1]})\n",
    "        scatter = axs[0].scatter(features_2d[:, 0], features_2d[:, 1], c=cluster_ids, cmap='tab10', s=15, alpha=0.8)\n",
    "        axs[0].set_title(f\"Class {class_id} - {class_name} - Cluster View\")\n",
    "        axs[0].set_xlabel(\"UMAP 1\")\n",
    "        axs[0].set_ylabel(\"UMAP 2\")\n",
    "        axs[0].grid(True)\n",
    "        fig.colorbar(scatter, ax=axs[0], ticks=np.unique(cluster_ids))\n",
    "        class_image = transforms.ToPILImage()(unnormalize_image(imgs[0]))\n",
    "        axs[1].imshow(class_image)\n",
    "        axs[1].set_title(f\"Sample\\n({class_name})\")\n",
    "        axs[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\" In {client} : Class {class_id} ({class_name}) Clustering with example\")\n",
    "        plt.close()\n",
    "        unique_clusters = np.unique(cluster_ids)\n",
    "        for cluster_id in unique_clusters:\n",
    "            mask = (cluster_ids == cluster_id)\n",
    "            cluster_feats = features[mask]\n",
    "            cluster_imgs = [imgs[i] for i in range(len(imgs)) if mask[i]]\n",
    "            centroid = np.mean(cluster_feats, axis=0)\n",
    "            dists = np.linalg.norm(cluster_feats - centroid, axis=1)\n",
    "            top_indices = np.argsort(dists)[:top_k_images]\n",
    "            fig_cluster, axs_cluster = plt.subplots(1, top_k_images, figsize=(top_k_images * 3, 3))\n",
    "            for i, idx in enumerate(top_indices):\n",
    "                img = cluster_imgs[idx]\n",
    "                img_for_display = transforms.ToPILImage()(unnormalize_image(img))\n",
    "                if not isinstance(axs_cluster, np.ndarray):\n",
    "                    axs_cluster = [axs_cluster]\n",
    "                axs_cluster[i].imshow(img_for_display)\n",
    "                axs_cluster[i].axis(\"off\")\n",
    "            plt.suptitle(f\"Class {class_id} ({class_name}) - Cluster {cluster_id}\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"In {client} : Class {class_id} ({class_name}) - Cluster {cluster_id}\")\n",
    "            plt.close()\n",
    "\n",
    "# Feature extraction\n",
    "@torch.no_grad()\n",
    "def extract_features_dinov2(dataloader, model, device=\"cuda\"):\n",
    "    try:\n",
    "        model = model.to(device).eval()\n",
    "        features, images, labels = [], [], []\n",
    "        for imgs, lbls in dataloader:\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            feats = model(imgs).cpu()\n",
    "            features.append(feats)\n",
    "            images.append(imgs.cpu())\n",
    "            labels.append(lbls.cpu())\n",
    "        features = torch.cat(features)\n",
    "        images = torch.cat(images)\n",
    "        labels = torch.cat(labels)\n",
    "        assert len(labels.shape) == 1, f\"Labels must be 1D, got shape {labels.shape}\"\n",
    "        assert images.shape[0] == labels.shape[0] == features.shape[0], \"Mismatched output sizes\"\n",
    "        torch.cuda.empty_cache()\n",
    "        return features, images, labels\n",
    "    except Exception as e:\n",
    "            print(f\"Client can't extract the Dino features because of : {e}\")\n",
    "    \n",
    "# Clustering\n",
    "def cluster_per_class(images, labels, features, sample_fraction=1.0, max_k=10):\n",
    "    client_clusters = defaultdict(dict)\n",
    "    data = defaultdict(list)\n",
    "    for img, lbl, feat in zip(images, labels, features):\n",
    "        data[lbl.item()].append((img, feat))\n",
    "        \n",
    "    for class_id, items in data.items():\n",
    "        if len(items) < 5:\n",
    "            print(f\"Skipping class {class_id}: only {len(items)} samples\")\n",
    "            continue\n",
    "        sample_count = max(int(len(items) * sample_fraction), 10)\n",
    "        items = items[:sample_count]\n",
    "        if len(items) < 4:\n",
    "            print(f\"Skipping class {class_id}: too few after sampling\")\n",
    "            continue\n",
    "        imgs = [x[0] for x in items]\n",
    "        feats = np.stack([x[1].numpy() for x in items])\n",
    "        features_pca = PCA(n_components=0.95).fit_transform(feats)\n",
    "        safe_n_neighbors = min(20, max(2, len(features_pca) - 1))\n",
    "        try:\n",
    "            features_umap = UMAP.UMAP(n_components=5, n_neighbors=safe_n_neighbors, random_state=42).fit_transform(feats)\n",
    "        except Exception as e:\n",
    "            print(f\"UMAP failed on class {class_id}: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Optimal K using Elbow Method to determine number of clusters per class    \n",
    "        inertias = []\n",
    "        k_range = list(range(2, min(max_k, len(features_umap)) + 1))\n",
    "        for k in k_range:\n",
    "            kmeans_try = KMeans(n_clusters=k, random_state=42).fit(features_umap)\n",
    "            inertias.append(kmeans_try.inertia_)\n",
    "        try:\n",
    "            elbow = KneeLocator(k_range, inertias, curve='convex', direction='decreasing').elbow\n",
    "            num_clusters = elbow if elbow is not None else 2\n",
    "        except Exception:\n",
    "            num_clusters = 2\n",
    "        \n",
    "        # Final KMeans with estimated k\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, random_state=42).fit(features_umap)\n",
    "            cluster_ids = kmeans.labels_\n",
    "        except Exception as e:\n",
    "            print(f\"KMeans failed on class {class_id}: {e}\")\n",
    "            continue\n",
    "        # Precompute 2D projection for visualization only    \n",
    "        features_2d = UMAP.UMAP(n_components=2, n_neighbors=safe_n_neighbors, random_state=42).fit_transform(feats)\n",
    "        client_clusters[class_id] = {\n",
    "            \"cluster_ids\": cluster_ids,\n",
    "            \"features\": features_pca,\n",
    "            \"features_2d\": features_2d,\n",
    "            \"features_raw\": feats,\n",
    "            \"umap\": features_umap,\n",
    "            \"images\": imgs\n",
    "        }\n",
    "    return client_clusters\n",
    "    \n",
    "def compute_saliency(img_tensor, model):\n",
    "    img_tensor = img_tensor.unsqueeze(0).cuda().requires_grad_()\n",
    "    output = model(img_tensor)\n",
    "    pred_class = output.argmax(dim=1)\n",
    "\n",
    "    loss = output[0, pred_class]\n",
    "    loss.backward()\n",
    "\n",
    "    saliency = img_tensor.grad.data.squeeze(0)  # (C, H, W)\n",
    "    saliency = saliency.mean(dim=0).cpu()  # Average over channels, keep negative/positive info\n",
    "    saliency = saliency.cpu()\n",
    "\n",
    "    # Robust normalization\n",
    "    vmax = torch.quantile(saliency.abs(), 0.99)  # 99th percentile\n",
    "    saliency = saliency / (vmax + 1e-10)  # scale using robust maximum\n",
    "    saliency = torch.clamp(saliency, -1, 1)\n",
    "\n",
    "    return saliency\n",
    "\n",
    "def overlay_saliency_on_image(image_tensor, saliency_map, alpha=0.5):\n",
    "    import matplotlib.cm as cm\n",
    "\n",
    "    # Convert image back to [0,1] range\n",
    "    image = unnormalize_image(image_tensor).cpu()\n",
    "    image = torch.clamp(image, 0, 1)\n",
    "\n",
    "    # Get saliency colormap\n",
    "    saliency_color = cm.RdYlGn((saliency_map.numpy() + 1) / 2.0)  # Normalize saliency to [0,1] for colormap\n",
    "    saliency_color = torch.tensor(saliency_color[..., :3]).permute(2,0,1)  # (C,H,W)\n",
    "\n",
    "    # Blend image and saliency\n",
    "    overlay = (1 - alpha) * image + alpha * saliency_color\n",
    "    overlay = torch.clamp(overlay, 0, 1)\n",
    "\n",
    "    return to_pil_image(overlay)\n",
    "\n",
    "\n",
    "def compute_tcav_score(cav, classwise_feats):\n",
    "    scores = {}\n",
    "    for class_id, feats in classwise_feats.items():\n",
    "        if len(feats) == 0:\n",
    "            scores[str(class_id)] = 0.0\n",
    "            continue\n",
    "        feats_norm_before = np.linalg.norm(feats)\n",
    "        #print(f\"[compute_tcav_score] feature value before normalization: {feats_norm_before:.4f}\")\n",
    "        feats_normalized = feats / (feats_norm_before + 1e-8)\n",
    "        feats_normalized_value=np.linalg.norm(feats_normalized)\n",
    "        #print(f\"[compute_tcav_score] feature value after normalization: {feats_normalized_value}\")\n",
    "        dot_products = np.dot(feats_normalized, cav)\n",
    "        tcav = float((dot_products > 0).mean())\n",
    "        scores[str(class_id)] = round(tcav, 4)\n",
    "        #print(f\"üîé Class {class_id}: mean dot={dot_products.mean():.4f}, std={dot_products.std():.4f}\")\n",
    "        #plt.hist(dot_products, bins=30)\n",
    "        #plt.title(f\"Dot Product Distribution: Class {class_id}\")\n",
    "       # plt.xlabel(\"dot(cav, feature)\")\n",
    "        #plt.ylabel(\"Count\")\n",
    "        #plt.show()\n",
    "    print(f\"[compute_tcav_score] TCAV scores -how relevance the concept to the class {class_id} -: {scores}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_classwise_feats_from_clusters(client_clusters):\n",
    "    return {\n",
    "        class_id: np.stack(data['features_raw']) for class_id, data in client_clusters.items()}\n",
    "\n",
    "def train_cluster_cav(cluster_feats, negative_feats):\n",
    "    X = np.vstack([cluster_feats, negative_feats])\n",
    "    y = np.array([1]*len(cluster_feats) + [0]*len(negative_feats))\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000).fit(X, y)\n",
    "    cav = clf.coef_[0]\n",
    "\n",
    "    norm_before = np.linalg.norm(cav)\n",
    "    cav_normalized = cav / (norm_before + 1e-8)\n",
    "    norm_after = np.linalg.norm(cav_normalized)\n",
    "    dim = cav.shape[0]\n",
    "    \n",
    "    print(f\"[train_cluster_cav] Positive samples: {len(cluster_feats)}, Negative samples: {len(negative_feats)}\")\n",
    "    #print(f\"[train_cluster_cav] CAV norm before normalization: {norm_before:.4f}\")\n",
    "    #print(f\"[train_cluster_cav] CAV norm after normalization: {norm_after:.4f}\")\n",
    "    #print(f\"[train_cluster_cav] CAV dimension -shape-: {dim}\")\n",
    "    return cav_normalized, clf  # normalized CAV\n",
    "\n",
    "def build_concept_signature(cluster_feats):\n",
    "    flat = cluster_feats.flatten()\n",
    "    signature = {\n",
    "        \"mean\": round(np.mean(flat), 5),\n",
    "        \"variance\": round(np.var(flat), 5),\n",
    "        \"skewness\": round(skew(flat), 5),\n",
    "        \"kurtosis\": round(kurtosis(flat), 5)\n",
    "    }\n",
    "    print(f\"[build_concept_signature] Concept signature: {signature}\")\n",
    "    return signature\n",
    "\n",
    "\n",
    "def process_clusters_for_tcav(client_clusters, classwise_feats, model, client_id=\"client_0\"):\n",
    "    concept_payloads = []\n",
    "    print(f\"\\nüîç [START] Processing clusters for TCAV (Client: {client_id})\")\n",
    "    total_clusters = 0\n",
    "    skipped_clusters = 0\n",
    "    \n",
    "    for class_id, cluster_data in client_clusters.items():\n",
    "        feats = np.array(cluster_data['features_raw'])\n",
    "        cluster_ids = np.array(cluster_data['cluster_ids'])\n",
    "        \n",
    "        for cluster_idx in np.unique(cluster_ids):\n",
    "            total_clusters += 1\n",
    "            pos_mask = (cluster_ids == cluster_idx)\n",
    "            pos_feats = feats[pos_mask]\n",
    "            cluster_size = len(pos_feats)\n",
    "            print(f\"  üî∏ Cluster {cluster_idx}: {cluster_size} samples\")\n",
    "            if len(pos_feats) < 2:\n",
    "                print(f\"‚è© Skipping cluster {cluster_idx} in class {class_id}: too few positives\")\n",
    "                skipped_clusters += 1\n",
    "                continue\n",
    "             # Intra-class negatives (other clusters in same class)\n",
    "            intra_mask = (cluster_ids != cluster_idx)\n",
    "            intra_neg_feats = feats[intra_mask]\n",
    "            \n",
    "            # Inter-class negatives (all other classes)\n",
    "            inter_neg_feats = [data['features_raw'] for other_class, data in client_clusters.items() if other_class != class_id]\n",
    "            inter_neg_feats = np.vstack(inter_neg_feats) if inter_neg_feats else np.empty((0, feats.shape[1]))\n",
    "            intra_count = len(pos_feats) // 2\n",
    "            inter_count = len(pos_feats) - intra_count\n",
    "            intra_sample = intra_neg_feats[np.random.choice(len(intra_neg_feats), size=intra_count, replace=True)] if len(intra_neg_feats) > 0 else np.empty((0, feats.shape[1]))\n",
    "            inter_sample = inter_neg_feats[np.random.choice(len(inter_neg_feats), size=inter_count, replace=True)] if len(inter_neg_feats) > 0 else np.empty((0, feats.shape[1]))\n",
    "            neg_feats = np.vstack([intra_sample, inter_sample])\n",
    "            if len(neg_feats) == 0:\n",
    "                print(f\"‚è© Skipping cluster {cluster_idx} in class {class_id}: too few positives\")\n",
    "                skipped_clusters += 1\n",
    "                continue\n",
    "            #  Train CAV\n",
    "            print(f\"Computing CAV...\")\n",
    "            cav, _ = train_cluster_cav(pos_feats, neg_feats)\n",
    "\n",
    "            # TCAV scoring\n",
    "            print(f\"Computing TCAV...\")\n",
    "            tcav_scores = compute_tcav_score(cav, classwise_feats)\n",
    "\n",
    "            #  Signature\n",
    "            print(f\"Computing gradient signature...\")\n",
    "            signature = build_concept_signature(pos_feats)\n",
    "            \n",
    "            concept_id = f\"{client_id}_class{class_id}_cluster{cluster_idx}\"\n",
    "            top_class = max(tcav_scores.items(), key=lambda x: x[1])[0]\n",
    "            concept_accuracy = float(top_class == str(class_id))\n",
    "            #purity = 1.0  # Placeholder; compute actual purity if labels are available per feature\n",
    "            #tcav_mean = float(np.mean(list(tcav_scores.values())))\n",
    "            #tcav_max = float(max(tcav_scores.values()))\n",
    "            concept_payloads.append({\n",
    "                \"concept_id\": concept_id,\n",
    "                \"concept_size\": len(pos_feats),\n",
    "                \"signature\": {k: float(v) for k, v in signature.items()},\n",
    "                \"tcav_scores\": {k: float(v) for k, v in tcav_scores.items()},\n",
    "                \"true_class\": str(class_id),\n",
    "                \"concept_accuracy\": concept_accuracy\n",
    "                #\"cluster_purity\": purity,\n",
    "                #\"tcav_mean\": tcav_mean,\n",
    "                #\"tcav_max\": tcav_max\n",
    "            })\n",
    "    return concept_payloads\n",
    "\n",
    "def map_local_concepts_to_global(local_concepts, global_dict, threshold=1):\n",
    "    mapping = {}\n",
    "    unmatched_local = []\n",
    "    unmatched_global = []\n",
    "    match_details = []\n",
    "\n",
    "    # Collect all GC signatures (already normalized)\n",
    "    gc_ids, gc_sigs = [], []\n",
    "    for gcid, info in global_dict.items():\n",
    "        sig = info[\"signature\"]\n",
    "        gc_ids.append(gcid)\n",
    "        gc_sigs.append([sig[\"mean\"], sig[\"variance\"], sig[\"skewness\"], sig[\"kurtosis\"]])\n",
    "\n",
    "    gc_signature_map = dict(zip(gc_ids, gc_sigs))\n",
    "\n",
    "    for local_concept in local_concepts:\n",
    "        local_sig = [\n",
    "            local_concept['signature']['mean'],\n",
    "            local_concept['signature']['variance'],\n",
    "            local_concept['signature']['skewness'],\n",
    "            local_concept['signature']['kurtosis']\n",
    "        ]\n",
    "\n",
    "        best_match = None\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for gcid, gc_sig in gc_signature_map.items():\n",
    "            dist = euclidean(local_sig, gc_sig)\n",
    "            if dist < best_score:\n",
    "                best_score = dist\n",
    "                best_match = gcid\n",
    "\n",
    "        if best_score <= threshold:\n",
    "            mapping[local_concept[\"concept_id\"]] = best_match\n",
    "            match_details.append({\n",
    "                \"local_concept\": local_concept[\"concept_id\"],\n",
    "                \"matched_global_concept\": best_match,\n",
    "                \"distance\": round(best_score, 4)\n",
    "            })\n",
    "        else:\n",
    "            unmatched_local.append(local_concept[\"concept_id\"])\n",
    "\n",
    "    matched_global = set(mapping.values())\n",
    "    unmatched_global = [gcid for gcid in global_dict if gcid not in matched_global]\n",
    "\n",
    "    print(f\"\\n[Client Matching] Matched: {len(mapping)}, Unmatched Local: {len(unmatched_local)}, Unmatched Global: {len(unmatched_global)}\")\n",
    "    print(\"\\nüîó Matched Pairs:\")\n",
    "    for d in match_details:\n",
    "        print(f\"  - {d['local_concept']} ‚Üî {d['matched_global_concept']} (distance={d['distance']})\")\n",
    "\n",
    "    return mapping, unmatched_local, unmatched_global\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca40d7-c2e3-42c9-a945-a3e7e558c72a",
   "metadata": {},
   "source": [
    "**FedCAPEClient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd6b90f-a43b-49de-b681-7d0e35840c34",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FedCAPENumpyClient(NumPyClient):\n",
    "    def __init__(self, cid, dataset, indices, model_path):\n",
    "        self.cid = f\"client_{cid}\"\n",
    "        self.cid_int=cid\n",
    "        self.dataset = Subset(dataset, indices)\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.concepts = []\n",
    "        self.global_dict = None\n",
    "        print(f\"[{self.cid}] Client initialized with {len(self.dataset)} samples.\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        if self.model is None:\n",
    "            print(f\"[{self.cid}] Loading model...\")\n",
    "            self.model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device).eval()\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        self._load_model()\n",
    "        return [val.cpu().detach().numpy() for val in self.model.parameters()]\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        self._load_model()\n",
    "        state_dict = dict(zip(self.model.state_dict().keys(), map(torch.tensor, parameters)))\n",
    "        self.model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    def analyze_global_local_differences(self, local_concepts, global_dict):\n",
    "        print(f\"[{self.cid}] üß© Analyzing alignment with global dictionary (threshold={1})\")\n",
    "        mapping, unmatched_local, unmatched_global = map_local_concepts_to_global(local_concepts, global_dict, threshold=0.5)\n",
    "\n",
    "        print(f\"[{self.cid}] ‚úÖ Alignment Summary:\")\n",
    "        print(f\"  ‚Ä¢ Total Local Concepts: {len(local_concepts)}\")\n",
    "        print(f\"  ‚Ä¢ Matched Concepts: {len(mapping)}\")\n",
    "        print(f\"  ‚Ä¢ Unmatched Local: {len(unmatched_local)} ‚Üí {unmatched_local[:5]}\")\n",
    "        print(f\"  ‚Ä¢ Unmatched Global: {len(unmatched_global)} ‚Üí {unmatched_global[:5]}\")\n",
    "\n",
    "        return mapping, unmatched_local, unmatched_global\n",
    "\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        try:\n",
    "            print(f\"[{self.cid}] üîß Starting fit.\")\n",
    "            self._load_model()\n",
    "            self.set_parameters(parameters)\n",
    "            \n",
    "            if \"global_dict\" in config and len(config[\"global_dict\"]) == 0:\n",
    "                print(f\"[{self.cid}] ‚ö†Ô∏è Warning: Global dictionary is empty!\")\n",
    "            if \"global_dict\" in config:\n",
    "                self.global_dict = json.loads(config[\"global_dict\"])\n",
    "                print(f\"[{self.cid}] ‚úÖ Received global_dict with {len(self.global_dict)} entries.\")\n",
    "            else:\n",
    "                print(f\"[{self.cid}] ‚ö†Ô∏è Warning: Global dictionary not found in config.\")\n",
    "                self.global_dict = {}\n",
    "            \n",
    "            loader = DataLoader(self.dataset, batch_size=128, shuffle=False)\n",
    "            features, images, labels = extract_features_dinov2(loader, self.model, device)\n",
    "            plot_real_client_distributions({0: client_indices[self.cid_int]}, num_clients=1)\n",
    "            clusters = cluster_per_class(images, labels, features, sample_fraction=1.0)\n",
    "            print (f\"printing Imagenet subnet classes Clustering for Client {self.cid}\")\n",
    "            visualize_clusters(clusters, dataset_classes=imagenet_subset.classes,client=self.cid,sample_per_class=500,top_k_images=5)\n",
    "            feats_by_class = get_classwise_feats_from_clusters(clusters)\n",
    "            self.concepts = process_clusters_for_tcav(clusters, feats_by_class, self.model, self.cid)\n",
    "            if self.concepts:\n",
    "                    self.local_to_global_mapping, unmatched_local, unmatched_global = self.analyze_global_local_differences(self.concepts, self.global_dict)\n",
    "            concepts_json = json.dumps(self.concepts)\n",
    "            print (f\"the fetched concepts Length is {len(concepts_json)}\")\n",
    "            print(f\"[{self.cid}] üîß Ending fit.\")\n",
    "            return self.get_parameters({}), len(self.dataset), {\"concepts\": concepts_json}\n",
    "        except Exception as e:\n",
    "            print(f\"[{self.cid}] ‚ùå Exception in fit(): {e}\")\n",
    "            raise\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        if not self.concepts and \"concepts\" in config:\n",
    "            self.concepts = json.loads(config[\"concepts\"])\n",
    "        if not self.concepts:\n",
    "            return 1.0, 1, {\"tcav_accuracy\": 0.0}\n",
    "        matches = total = 0\n",
    "        for concept in self.concepts:\n",
    "            try:\n",
    "                concept_id = concept.get(\"concept_id\", \"\")\n",
    "                tcav_scores = concept.get(\"tcav_scores\", {})\n",
    "                if not tcav_scores:\n",
    "                    continue\n",
    "                true_class = concept.get(\"true_class\", concept_id.split(\"_class\")[1].split(\"_\")[0])\n",
    "                concept_accuracy = concept.get(\"concept_accuracy\", 0.0)\n",
    "                if concept_accuracy > 0.5:  # Threshold for correct prediction\n",
    "                    matches += 1\n",
    "                total += 1\n",
    "            except Exception:\n",
    "                continue\n",
    "        accuracy = matches / max(total, 1)\n",
    "        loss = 1.0 - accuracy\n",
    "        return loss, total, {\"tcav_accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40357ef-5789-41f8-8120-4d7b2dfcb8d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**FedCAPEServer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3410bb-a45f-4db5-9a9a-6b69a6ab763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FedCAPE Global Concept Matching with Ordered TCAV and Debuggable Output\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "class FederatedConceptServer:\n",
    "    def __init__(self, similarity_threshold=1): # meaning the Metrics Vectors are within ~1 standard deviation\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def _extract_vectors(self, concept):\n",
    "        sig = concept[\"signature\"]\n",
    "        return np.array([sig[\"mean\"], sig[\"variance\"], sig[\"skewness\"], sig[\"kurtosis\"]], dtype=np.float32)\n",
    "\n",
    "    def extract_metrics(self, payloads):\n",
    "        metrics, metadata = [], []\n",
    "        for payload in payloads:\n",
    "            sig = payload['signature']\n",
    "            metrics.append([sig['mean'], sig['variance'], sig['skewness'], sig['kurtosis']])\n",
    "            metadata.append({\n",
    "                'concept_id': payload['concept_id'],\n",
    "                'concept_size': payload['concept_size'],\n",
    "                'tcav_scores': payload['tcav_scores'],\n",
    "                'concept_accuracy': payload.get('concept_accuracy', 0.0),\n",
    "                'true_class': payload.get('true_class', payload['concept_id'].split('_class')[1].split('_')[0]),\n",
    "                'client_id': payload['concept_id'].split('_')[0]\n",
    "            })\n",
    "        return np.array(metrics), metadata\n",
    "\n",
    "    def normalize_metrics(self, metrics):\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(metrics), scaler\n",
    "\n",
    "    def compute_similarities(self, metrics_0, metrics_1, meta_0, meta_1, threshold):\n",
    "        global_concepts = []\n",
    "        all_candidate_pairs = []\n",
    "\n",
    "        for i, (m0, meta0) in enumerate(zip(metrics_0, meta_0)):\n",
    "            pred_0 = max(meta0['tcav_scores'], key=meta0['tcav_scores'].get)\n",
    "            if str(pred_0) != str(meta0['true_class']):\n",
    "                continue\n",
    "            for j, (m1, meta1) in enumerate(zip(metrics_1, meta_1)):\n",
    "                pred_1 = max(meta1['tcav_scores'], key=meta1['tcav_scores'].get)\n",
    "                if str(pred_1) != str(meta1['true_class']):\n",
    "                    continue\n",
    "                distance = euclidean(m0, m1)\n",
    "                if distance <= threshold:\n",
    "                    all_candidate_pairs.append((distance, i, j, m0, m1, meta0, meta1))\n",
    "\n",
    "        all_candidate_pairs.sort(key=lambda x: x[0])\n",
    "        used_0 = set()\n",
    "        used_1 = set()\n",
    "\n",
    "        for distance, i, j, m0, m1, meta0, meta1 in all_candidate_pairs:\n",
    "            if i in used_0 or j in used_1:\n",
    "                continue\n",
    "            used_0.add(i)\n",
    "            used_1.add(j)\n",
    "            all_classes = set(meta0['tcav_scores']) | set(meta1['tcav_scores'])\n",
    "            dominant_class = max(\n",
    "                {k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2 for k in all_classes},\n",
    "                key=lambda k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2\n",
    "            )\n",
    "            global_concepts.append({\n",
    "                'global_concept_id': str(uuid.uuid4()),\n",
    "                'signature': {\n",
    "                    'mean': float((m0[0] + m1[0]) / 2),\n",
    "                    'variance': float((m0[1] + m1[1]) / 2),\n",
    "                    'skewness': float((m0[2] + m1[2]) / 2),\n",
    "                    'kurtosis': float((m0[3] + m1[3]) / 2),\n",
    "                },\n",
    "                'tcav_scores': {\n",
    "                    k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2\n",
    "                    for k in all_classes\n",
    "                },\n",
    "                'dominant_class': str(dominant_class),\n",
    "                'contributing_concepts': [\n",
    "                    {\n",
    "                        'client_id': meta0['client_id'],\n",
    "                        'concept_id': meta0['concept_id'],\n",
    "                        'concept_size': meta0['concept_size'],\n",
    "                        'tcav_scores': meta0['tcav_scores'],\n",
    "                        'concept_accuracy': meta0['concept_accuracy']\n",
    "                    },\n",
    "                    {\n",
    "                        'client_id': meta1['client_id'],\n",
    "                        'concept_id': meta1['concept_id'],\n",
    "                        'concept_size': meta1['concept_size'],\n",
    "                        'tcav_scores': meta1['tcav_scores'],\n",
    "                        'concept_accuracy': meta1['concept_accuracy']\n",
    "                    }\n",
    "                ],\n",
    "                'distance': float(distance)\n",
    "            })\n",
    "\n",
    "        return global_concepts\n",
    "\n",
    "    def evaluate_global_dictionary(self, global_dict):\n",
    "        total_groups = len(global_dict)\n",
    "        pure_groups = total_members = 0\n",
    "        mismatch_samples = []\n",
    "\n",
    "        for gc_id, gc_info in global_dict.items():\n",
    "            dominant = str(gc_info['dominant_class'])\n",
    "            true_classes = [str(cid.split(\"_class\")[1].split(\"_\")[0]) for cid in gc_info['concept_ids']]\n",
    "            total_members += len(true_classes)\n",
    "            if all(cls == dominant for cls in true_classes):\n",
    "                pure_groups += 1\n",
    "            else:\n",
    "                mismatch_samples.append({\"gc_id\": gc_id, \"distribution\": dict(Counter(true_classes))})\n",
    "\n",
    "        purity = pure_groups / total_groups if total_groups > 0 else 0\n",
    "        avg_members = total_members / total_groups if total_groups > 0 else 0\n",
    "        print(f\"\\nüìä [Global Matching Evaluation]: Total={total_groups}, Avg Members={avg_members:.2f}, Purity={purity*100:.2f}%\")\n",
    "        if mismatch_samples:\n",
    "            print(\"\\n‚ö° Mixed-class groups:\")\n",
    "            for m in mismatch_samples[:5]:\n",
    "                print(f\"  - {m['gc_id']}: {m['distribution']}\")\n",
    "\n",
    "    def receive_concepts(self, client_concepts_list):\n",
    "        if len(client_concepts_list) < 2:\n",
    "            raise ValueError(\"Need at least two clients to align concepts\")\n",
    "\n",
    "        all_metrics, all_metadata = [], []\n",
    "        for concepts in client_concepts_list:\n",
    "            metrics, meta = self.extract_metrics(concepts)\n",
    "            all_metrics.append(metrics)\n",
    "            all_metadata.append(meta)\n",
    "\n",
    "        combined_metrics = np.vstack(all_metrics)\n",
    "        normalized_metrics, _ = self.normalize_metrics(combined_metrics)\n",
    "\n",
    "        split_0 = normalized_metrics[:len(all_metrics[0])]\n",
    "        split_1 = normalized_metrics[len(all_metrics[0]):]\n",
    "\n",
    "        global_concepts = self.compute_similarities(\n",
    "            split_0, split_1, all_metadata[0], all_metadata[1], threshold=self.similarity_threshold\n",
    "        )\n",
    "\n",
    "        ordered_all = sorted(global_concepts, key=lambda gc: max(gc['tcav_scores'].values()), reverse=True)\n",
    "        print(\"\\nüåê Top 5 contributing global concepts (by TCAV max score):\")\n",
    "        for gc in ordered_all[:5]:\n",
    "            print(f\"  - Global Concept: {gc['global_concept_id']} (TCAV max={max(gc['tcav_scores'].values()):.3f})\")\n",
    "            for contrib in gc['contributing_concepts']:\n",
    "                print(f\"    - {contrib['concept_id']} from {contrib['client_id']} (size={contrib['concept_size']})\")\n",
    "                for cls, score in contrib['tcav_scores'].items():\n",
    "                    print(f\"        ‚Üí Class {cls} TCAV score: {score:.4f}\")\n",
    "                print(f\"        ‚Ä¢ Concept Accuracy: {contrib['concept_accuracy']:.3f}\")\n",
    "\n",
    "        global_dict = {}\n",
    "        for i, gc in enumerate(global_concepts):\n",
    "            concept_ids = [c['concept_id'] for c in gc['contributing_concepts']]\n",
    "            global_dict[f\"GC_{i}\"] = {\n",
    "                \"global_concept_id\": gc[\"global_concept_id\"],\n",
    "                \"signature\": gc[\"signature\"],\n",
    "                \"tcav_scores\": gc[\"tcav_scores\"],\n",
    "                \"dominant_class\": gc[\"dominant_class\"],\n",
    "                \"contributing_concepts\": gc[\"contributing_concepts\"],\n",
    "                \"concept_ids\": concept_ids,\n",
    "                \"distance\": gc[\"distance\"]\n",
    "            }\n",
    "\n",
    "        self.evaluate_global_dictionary(global_dict)\n",
    "\n",
    "        class_to_concepts = defaultdict(list)\n",
    "        for gc in global_dict.values():\n",
    "            cls = gc.get('dominant_class')\n",
    "            score = gc['tcav_scores'].get(cls, 0.0)\n",
    "            class_to_concepts[cls].append((gc['global_concept_id'], score))\n",
    "\n",
    "        print(\"\\n‚úÖ Available Classes:\", sorted(class_to_concepts.keys()))\n",
    "        print(\"\\nüìò Detailed Global Concepts by Class:\")\n",
    "        for cls, concept_scores in class_to_concepts.items():\n",
    "            print(f\"\\nüîπ Class {cls}:\")\n",
    "            top_concepts = sorted(concept_scores, key=lambda x: x[1], reverse=True)\n",
    "            for gc_id, tcav in top_concepts:\n",
    "                gc = next((g for g in global_concepts if g['global_concept_id'] == gc_id), None)\n",
    "                if not gc:\n",
    "                    continue\n",
    "                print(f\"  üì¶ Global Concept {gc_id}\")\n",
    "                print(f\"     ‚Ä¢ Signature: Œº={gc['signature']['mean']:.5f}, œÉ¬≤={gc['signature']['variance']:.5f}, skew={gc['signature']['skewness']:.5f}, kurt={gc['signature']['kurtosis']:.5f}\")\n",
    "                print(f\"     ‚Ä¢ TCAV scores:\")\n",
    "                for k, v in gc['tcav_scores'].items():\n",
    "                    print(f\"       - Class {k}: {v:.4f}\")\n",
    "                print(f\"     ‚Ä¢ Contributors:\")\n",
    "                for contrib in gc['contributing_concepts']:\n",
    "                    print(f\"       - {contrib['client_id']}::{contrib['concept_id']} (size={contrib['concept_size']})\")\n",
    "                    for k, v in contrib['tcav_scores'].items():\n",
    "                        print(f\"           ‚Üí Class {k}: {v:.4f}\")\n",
    "                    print(f\"           ‚Ä¢ Concept Accuracy: {contrib['concept_accuracy']:.3f}\")\n",
    "                print(f\"     ‚Ä¢ Distance between contributors: {gc['distance']:.4f}\")\n",
    "\n",
    "        print(\"\\nüìå Global Concepts per Class (Top Contributors):\")\n",
    "        for cls, concept_scores in class_to_concepts.items():\n",
    "            top_concepts = sorted(concept_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"  ‚Üí Class {cls}:\")\n",
    "            for gc_id, score in top_concepts:\n",
    "                print(f\"      - {gc_id}: TCAV={score:.4f}\")\n",
    "\n",
    "        with open(\"class_concept_map.json\", \"w\") as f:\n",
    "            json.dump({cls: sorted(concept_scores, key=lambda x: x[1], reverse=True)\n",
    "                       for cls, concept_scores in class_to_concepts.items()}, f, indent=2)\n",
    "\n",
    "        return global_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f21124-eb67-4e74-8b3f-eb3fec7f2247",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FedCAPE Global Concept Matching with Ordered TCAV and Debuggable Output\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import uuid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "\n",
    "class FederatedConceptServer:\n",
    "    def __init__(self, similarity_threshold=1):   # meaning the Metrics Vectors are within ~1 standard deviation\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "\n",
    "    def _extract_vectors(self, concept):\n",
    "        sig = concept[\"signature\"]\n",
    "        return np.array([sig[\"mean\"], sig[\"variance\"], sig[\"skewness\"], sig[\"kurtosis\"]], dtype=np.float32)\n",
    "\n",
    "    def extract_metrics(self, payloads):\n",
    "        metrics, metadata = [], []\n",
    "        for payload in payloads:\n",
    "            sig = payload['signature']\n",
    "            metrics.append([sig['mean'], sig['variance'], sig['skewness'], sig['kurtosis']])\n",
    "            metadata.append({\n",
    "                'concept_id': payload['concept_id'],\n",
    "                'concept_size': payload['concept_size'],\n",
    "                'tcav_scores': payload['tcav_scores'],\n",
    "                'concept_accuracy': payload.get('concept_accuracy', 0.0),\n",
    "                'true_class': payload.get('true_class', payload['concept_id'].split('_class')[1].split('_')[0]),\n",
    "                'client_id': payload['concept_id'].split('_')[0]\n",
    "            })\n",
    "        return np.array(metrics), metadata\n",
    "\n",
    "    def normalize_metrics(self, metrics):\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(metrics), scaler\n",
    "\n",
    "    def compute_similarities(self, metrics_0, metrics_1, meta_0, meta_1, threshold):\n",
    "        global_concepts = []\n",
    "        seen_pairs = set()\n",
    "        used_concepts = set()\n",
    "\n",
    "        for i, (m0, meta0) in enumerate(zip(metrics_0, meta_0)):\n",
    "            pred_0 = max(meta0['tcav_scores'], key=meta0['tcav_scores'].get)\n",
    "            if str(pred_0) != str(meta0['true_class']) or meta0['concept_id'] in used_concepts:\n",
    "                continue\n",
    "\n",
    "            for j, (m1, meta1) in enumerate(zip(metrics_1, meta_1)):\n",
    "                pair_key = tuple(sorted([meta0['concept_id'], meta1['concept_id']]))\n",
    "                if pair_key in seen_pairs or meta1['concept_id'] in used_concepts:\n",
    "                    continue\n",
    "                seen_pairs.add(pair_key)\n",
    "\n",
    "                pred_1 = max(meta1['tcav_scores'], key=meta1['tcav_scores'].get)\n",
    "                if str(pred_1) != str(meta1['true_class']):\n",
    "                    continue\n",
    "\n",
    "                distance = euclidean(m0, m1)\n",
    "                if distance <= threshold:\n",
    "                    all_classes = set(meta0['tcav_scores']) | set(meta1['tcav_scores'])\n",
    "                    used_concepts.update([meta0['concept_id'], meta1['concept_id']])\n",
    "                    dominant_class = max(\n",
    "                        {k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2 for k in all_classes},\n",
    "                        key=lambda k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2\n",
    "                    )\n",
    "                    global_concepts.append({\n",
    "                        'global_concept_id': str(uuid.uuid4()),\n",
    "                        'signature': {\n",
    "                            'mean': float((m0[0] + m1[0]) / 2),\n",
    "                            'variance': float((m0[1] + m1[1]) / 2),\n",
    "                            'skewness': float((m0[2] + m1[2]) / 2),\n",
    "                            'kurtosis': float((m0[3] + m1[3]) / 2),\n",
    "                        },\n",
    "                        'tcav_scores': {\n",
    "                            k: (meta0['tcav_scores'].get(k, 0.0) + meta1['tcav_scores'].get(k, 0.0)) / 2\n",
    "                            for k in all_classes\n",
    "                        },\n",
    "                        'dominant_class': str(dominant_class),\n",
    "                        'contributing_concepts': [\n",
    "                            {\n",
    "                                'client_id': meta0['client_id'],\n",
    "                                'concept_id': meta0['concept_id'],\n",
    "                                'concept_size': meta0['concept_size'],\n",
    "                                'tcav_scores': meta0['tcav_scores'],\n",
    "                                'concept_accuracy': meta0['concept_accuracy']\n",
    "                            },\n",
    "                            {\n",
    "                                'client_id': meta1['client_id'],\n",
    "                                'concept_id': meta1['concept_id'],\n",
    "                                'concept_size': meta1['concept_size'],\n",
    "                                'tcav_scores': meta1['tcav_scores'],\n",
    "                                'concept_accuracy': meta1['concept_accuracy']\n",
    "                            }\n",
    "                        ],\n",
    "                        'distance': float(distance)\n",
    "                    })\n",
    "        return global_concepts\n",
    "\n",
    "    def evaluate_global_dictionary(self, global_dict):\n",
    "        total_groups = len(global_dict)\n",
    "        pure_groups = total_members = 0\n",
    "        mismatch_samples = []\n",
    "\n",
    "        for gc_id, gc_info in global_dict.items():\n",
    "            dominant = str(gc_info['dominant_class'])\n",
    "            true_classes = [str(cid.split(\"_class\")[1].split(\"_\")[0]) for cid in gc_info['concept_ids']]\n",
    "            total_members += len(true_classes)\n",
    "            if all(cls == dominant for cls in true_classes):\n",
    "                pure_groups += 1\n",
    "            else:\n",
    "                mismatch_samples.append({\"gc_id\": gc_id, \"distribution\": dict(Counter(true_classes))})\n",
    "\n",
    "        purity = pure_groups / total_groups if total_groups > 0 else 0\n",
    "        avg_members = total_members / total_groups if total_groups > 0 else 0\n",
    "        print(f\"\\nüìä [Global Matching Evaluation]: Total={total_groups}, Avg Members={avg_members:.2f}, Purity={purity*100:.2f}%\")\n",
    "        if mismatch_samples:\n",
    "            print(\"\\n‚ö° Mixed-class groups:\")\n",
    "            for m in mismatch_samples[:5]:\n",
    "                print(f\"  - {m['gc_id']}: {m['distribution']}\")\n",
    "\n",
    "    def receive_concepts(self, client_concepts_list):\n",
    "        if len(client_concepts_list) < 2:\n",
    "            raise ValueError(\"Need at least two clients to align concepts\")\n",
    "\n",
    "        all_metrics, all_metadata = [], []\n",
    "        for concepts in client_concepts_list:\n",
    "            metrics, meta = self.extract_metrics(concepts)\n",
    "            all_metrics.append(metrics)\n",
    "            all_metadata.append(meta)\n",
    "\n",
    "        combined_metrics = np.vstack(all_metrics)\n",
    "        normalized_metrics, _ = self.normalize_metrics(combined_metrics)\n",
    "\n",
    "        split_0 = normalized_metrics[:len(all_metrics[0])]\n",
    "        split_1 = normalized_metrics[len(all_metrics[0]):]\n",
    "\n",
    "        global_concepts = self.compute_similarities(\n",
    "            split_0, split_1, all_metadata[0], all_metadata[1], threshold=self.similarity_threshold\n",
    "        )\n",
    "\n",
    "        ordered_all = sorted(global_concepts, key=lambda gc: max(gc['tcav_scores'].values()), reverse=True)\n",
    "        print(\"\\nüåê Top 5 contributing global concepts (by TCAV max score):\")\n",
    "        for gc in ordered_all[:5]:\n",
    "            print(f\"  - Global Concept: {gc['global_concept_id']} (TCAV max={max(gc['tcav_scores'].values()):.3f})\")\n",
    "            for contrib in gc['contributing_concepts']:\n",
    "                print(f\"    - {contrib['concept_id']} from {contrib['client_id']} (size={contrib['concept_size']})\")\n",
    "                for cls, score in contrib['tcav_scores'].items():\n",
    "                    print(f\"        ‚Üí Class {cls} TCAV score: {score:.4f}\")\n",
    "                print(f\"        ‚Ä¢ Concept Accuracy: {contrib['concept_accuracy']:.3f}\")\n",
    "\n",
    "        global_dict = {}\n",
    "        for i, gc in enumerate(global_concepts):\n",
    "            concept_ids = [c['concept_id'] for c in gc['contributing_concepts']]\n",
    "            global_dict[f\"GC_{i}\"] = {\n",
    "                \"global_concept_id\": gc[\"global_concept_id\"],\n",
    "                \"signature\": gc[\"signature\"],\n",
    "                \"tcav_scores\": gc[\"tcav_scores\"],\n",
    "                \"dominant_class\": gc[\"dominant_class\"],\n",
    "                \"contributing_concepts\": gc[\"contributing_concepts\"],\n",
    "                \"concept_ids\": concept_ids,\n",
    "                \"distance\": gc[\"distance\"]\n",
    "            }\n",
    "\n",
    "        self.evaluate_global_dictionary(global_dict)\n",
    "\n",
    "        class_to_concepts = defaultdict(list)\n",
    "        for gc in global_dict.values():\n",
    "            cls = gc.get('dominant_class')\n",
    "            score = gc['tcav_scores'].get(cls, 0.0)\n",
    "            class_to_concepts[cls].append((gc['global_concept_id'], score))\n",
    "\n",
    "        print(\"\\n‚úÖ Available Classes:\", sorted(class_to_concepts.keys()))\n",
    "        print(\"\\nüìò Detailed Global Concepts by Class:\")\n",
    "        for cls, concept_scores in class_to_concepts.items():\n",
    "            print(f\"\\nüîπ Class {cls}:\")\n",
    "            top_concepts = sorted(concept_scores, key=lambda x: x[1], reverse=True)\n",
    "            for gc_id, tcav in top_concepts:\n",
    "                gc = next((g for g in global_concepts if g['global_concept_id'] == gc_id), None)\n",
    "                if not gc:\n",
    "                    continue\n",
    "                print(f\"  üì¶ Global Concept {gc_id}\")\n",
    "                print(f\"     ‚Ä¢ Signature: Œº={gc['signature']['mean']:.5f}, œÉ¬≤={gc['signature']['variance']:.5f}, skew={gc['signature']['skewness']:.5f}, kurt={gc['signature']['kurtosis']:.5f}\")\n",
    "                print(f\"     ‚Ä¢ TCAV scores:\")\n",
    "                for k, v in gc['tcav_scores'].items():\n",
    "                    print(f\"       - Class {k}: {v:.4f}\")\n",
    "                print(f\"     ‚Ä¢ Contributors:\")\n",
    "                for contrib in gc['contributing_concepts']:\n",
    "                    print(f\"       - {contrib['client_id']}::{contrib['concept_id']} (size={contrib['concept_size']})\")\n",
    "                    for k, v in contrib['tcav_scores'].items():\n",
    "                        print(f\"           ‚Üí Class {k}: {v:.4f}\")\n",
    "                    print(f\"           ‚Ä¢ Concept Accuracy: {contrib['concept_accuracy']:.3f}\")\n",
    "                print(f\"     ‚Ä¢ Distance between contributors: {gc['distance']:.4f}\")\n",
    "\n",
    "        print(\"\\nüìå Global Concepts per Class (Top Contributors):\")\n",
    "        for cls, concept_scores in class_to_concepts.items():\n",
    "            top_concepts = sorted(concept_scores, key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"  ‚Üí Class {cls}:\")\n",
    "            for gc_id, score in top_concepts:\n",
    "                print(f\"      - {gc_id}: TCAV={score:.4f}\")\n",
    "\n",
    "        with open(\"class_concept_map.json\", \"w\") as f:\n",
    "            json.dump({cls: sorted(concept_scores, key=lambda x: x[1], reverse=True)\n",
    "                       for cls, concept_scores in class_to_concepts.items()}, f, indent=2)\n",
    "\n",
    "        return global_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d95f8-7144-407b-8251-186680ccbe45",
   "metadata": {},
   "source": [
    "**FedCAPEStrategy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19cac325-e847-46ed-a806-1ea09f7a554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedCAPEStrategy(FedAvg):\n",
    "    def __init__(self, initial_parameters, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.initial_parameters = initial_parameters\n",
    "        self.server = FederatedConceptServer(similarity_threshold=1)  # meaning the Metrics Vectors are within ~1 standard deviation \n",
    "        self.global_dict = {}\n",
    "        self.client_concept_map = {}\n",
    "\n",
    "    def initialize_parameters(self, client_manager):\n",
    "        return self.initial_parameters\n",
    "\n",
    "    def aggregate_fit(self, server_round, results, failures):\n",
    "        if failures:\n",
    "            print(f\"‚ö†Ô∏è Failures in round {server_round}: {failures}\")\n",
    "        client_concepts = []\n",
    "        self.client_concept_map = {}\n",
    "        for client, res in results:\n",
    "            cid = client.cid\n",
    "            raw_json = res.metrics.get(\"concepts\", \"[]\")\n",
    "            parsed = json.loads(raw_json)\n",
    "            client_concepts.append(parsed)\n",
    "            self.client_concept_map[cid] = parsed\n",
    "        if not client_concepts:\n",
    "            print(\"‚ùå No valid concepts received.\")\n",
    "            _, first_result = results[0]\n",
    "            return first_result.parameters, {}\n",
    "        self.global_dict = self.server.receive_concepts(client_concepts)\n",
    "        print(f\"[Server] ‚úÖ Global dictionary built with {len(self.global_dict)} entries.\")\n",
    "        _, first_res = results[0]\n",
    "        return first_res.parameters, {}\n",
    "\n",
    "    def configure_fit(self, server_round, parameters, client_manager):\n",
    "        config = {\"global_dict\": json.dumps(self.global_dict)}\n",
    "        fit_ins = FitIns(parameters, config)\n",
    "        return [(client, fit_ins) for client in client_manager.all().values()]\n",
    "\n",
    "    def configure_evaluate(self, server_round, parameters, client_manager):\n",
    "        evaluate_ins = []\n",
    "        for client in client_manager.all().values():\n",
    "            cid = client.cid\n",
    "            concept_json = json.dumps(self.client_concept_map.get(cid, []))\n",
    "            config = {\"concepts\": concept_json}\n",
    "            evaluate_ins.append((client, EvaluateIns(parameters, config)))\n",
    "        return evaluate_ins\n",
    "\n",
    "    def aggregate_evaluate(self, server_round, results, failures):\n",
    "        if not results:\n",
    "            return 1.0, {\"tcav_accuracy\": 0.0}\n",
    "        total = sum(res.num_examples for _, res in results)\n",
    "        avg_acc = sum(res.num_examples * float(res.metrics.get(\"tcav_accuracy\", 0.0)) for _, res in results)\n",
    "        avg_accuracy = avg_acc / max(total, 1)\n",
    "        print(f\"üìä [Server] TCAV Accuracy: {avg_accuracy:.4f}\")\n",
    "        return 1.0 - avg_accuracy, {\"tcav_accuracy\": round(avg_accuracy, 4)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb135e4-b3fe-4729-828f-a69c5c69c08f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**FedCAPESimulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad899c88-7c02-498b-883e-0eb604a739cf",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# determining number of clients and prepare the data distribution \n",
    "NUM_CLIENTS = 2\n",
    "client_indices = create_dirichlet_clients(imagenet_subset, num_clients=NUM_CLIENTS, alpha=1)\n",
    "print(\"ImageNet-style dataset loaded and Assigned to client\")\n",
    "def client_fn(context: Context):\n",
    "    cid = int(context.node_config['partition-id'])\n",
    "    return FedCAPENumpyClient(cid, imagenet_subset, client_indices[cid], model_path=\"\").to_client()\n",
    "\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "initial_parameters = ndarrays_to_parameters([p.detach().cpu().numpy() for p in model.parameters()])\n",
    "strategy = FedCAPEStrategy(\n",
    "    initial_parameters=initial_parameters,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS\n",
    ")\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=NUM_CLIENTS,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=strategy,\n",
    "    client_resources={\"num_cpus\": 2, \"num_gpus\": 1},\n",
    "    ray_init_args={\"runtime_env\": {\"pip\": [\"torch\", \"flwr\", \"xformers\", \"umap-learn\", \"kneed\", \"scikit-learn\"]}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7af2e8-2b14-4554-8d6e-1214bfa3d9ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
